

<!doctype html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>quantificationlib.multiclass package &#8212; Prueba QLIB Jaime 0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/bizstyle.css" />
    
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/bizstyle.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0" />
    <!--[if lt IE 9]>
    <script src="_static/css3-mediaqueries.js"></script>
    <![endif]-->
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">Prueba QLIB Jaime 0.1 documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">quantificationlib.multiclass package</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="quantificationlib-multiclass-package">
<h1>quantificationlib.multiclass package<a class="headerlink" href="#quantificationlib-multiclass-package" title="Permalink to this heading">¶</a></h1>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this heading">¶</a></h2>
</section>
<section id="module-quantificationlib.multiclass.df">
<span id="quantificationlib-multiclass-df-module"></span><h2>quantificationlib.multiclass.df module<a class="headerlink" href="#module-quantificationlib.multiclass.df" title="Permalink to this heading">¶</a></h2>
<p>Multiclass versions for quantifiers based on representing the distributions using CDFs/PDFs</p>
<dl class="py class">
<dt class="sig sig-object py" id="quantificationlib.multiclass.df.DFX">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">DFX</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">distribution_function</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'PDF'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_bins</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bin_strategy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'equal_width'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'HD'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/quantificationlib/multiclass/df.html#DFX"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#quantificationlib.multiclass.df.DFX" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="rsts_t4/quantificationlib.base.html#quantificationlib.base.WithoutClassifiers" title="quantificationlib.base.WithoutClassifiers"><code class="xref py py-class docutils literal notranslate"><span class="pre">WithoutClassifiers</span></code></a></p>
<p>Generic Multiclass DFX method</p>
<blockquote>
<div><p>The idea is to represent the mixture of the training distribution and the testing distribution
(using CDFs/PDFs) of the features of the input space (X). The difference between both are minimized using a
distante/loss function. Originally, (González et al. 2013) propose the combination of PDF and
Hellinger Distance, but also CDF and any other distance/loss function could be used, like L1 or L2.</p>
<p>The class has two parameters to select:</p>
<ul class="simple">
<li><p>the method used to represent the distributions (CDFs or PDFs)</p></li>
<li><p>the distance used.</p></li>
</ul>
<dl>
<dt>distribution_function<span class="classifier">str, (default=’PDF’)</span></dt><dd><p>Type of distribution function used. Two types are supported ‘CDF’ and ‘PDF’</p>
</dd>
<dt>n_bins<span class="classifier">int</span></dt><dd><p>Number of bins to compute the PDFs</p>
</dd>
<dt>bin_strategy<span class="classifier">str (default=’norm’)</span></dt><dd><dl>
<dt>Method to compute the boundaries of the bins:</dt><dd><p>‘equal_width’: bins of equal length (it could be affected by outliers)
‘equal_count’: bins of equal counts (considering the examples of all classes)
‘binormal’: (Only for binary quantification) It is inspired on the method devised by</p>
<blockquote>
<div><p>(Tasche, 2019, Eq (A16b)). the cut points, $-infty &lt; c_1 &lt; ldots &lt; c_{b-1} &lt; infty$,
are computed as follows based on the assumption that the features follow a normal distribution:</p>
<p>$c_i =</p>
</div></blockquote>
</dd>
</dl>
</dd>
</dl>
</div></blockquote>
<p>rac{sigma^+ + sigma^{-}}{2} Phi^{-1}igg(
rac{i}{b}igg)  +
rac{mu^+ + mu^{-}}{2} ,  quad i=1,ldots,b-1$</p>
<blockquote>
<div><blockquote>
<div><p>where $Phi^{-1}$ is the quantile function of the standard normal distribution, and $mu$
and $sigma$ of the normal distribution are estimated as the average of those values for
the training examples of each class.</p>
</div></blockquote>
<dl>
<dt>‘normal’:  The idea is that each feacture follows a normal distribution. $mu$ and $sigma$ are</dt><dd><p>estimated as the weighted mean and std from the training distribution. The cut points
$-infty &lt; c_1 &lt; ldots &lt; c_{b-1} &lt; infty$ are computed as follows:</p>
<p>$c_i = sigma^ Phi^{-1}igg(</p>
</dd>
</dl>
</div></blockquote>
<p>rac{i}{b}igg)  + mu ,  quad i=1,ldots,b-1$</p>
<blockquote>
<div><dl class="simple">
<dt>distance<span class="classifier">str, representing the distance function (default=’HD’)</span></dt><dd><p>It is the name of the distance used to compute the difference between the mixture of the training
distribution and the testing distribution</p>
</dd>
<dt>tol<span class="classifier">float, (default=1e-05)</span></dt><dd><p>The precision of the solution when search is used to compute the prevalence</p>
</dd>
<dt>verbose<span class="classifier">int, optional, (default=0)</span></dt><dd><p>The verbosity level. The default value, zero, means silent mode</p>
</dd>
</dl>
<dl class="simple">
<dt><a href="#id7"><span class="problematic" id="id8">classes_</span></a><span class="classifier">ndarray, shape (n_classes, )</span></dt><dd><p>Class labels</p>
</dd>
<dt>distribution_function<span class="classifier">str</span></dt><dd><p>Type of distribution function used. Two types are supported ‘CDF’ and ‘PDF’</p>
</dd>
<dt>n_bins<span class="classifier">int</span></dt><dd><p>The number of bins to compute the PDFs</p>
</dd>
<dt>bin_strategy<span class="classifier">str</span></dt><dd><p>Method to compute the boundaries of the bins</p>
</dd>
<dt>distance<span class="classifier">str or a distance function</span></dt><dd><p>A string with the name of the distance function (‘HD’/’L1’/’L2’) or a distance function</p>
</dd>
<dt>tol<span class="classifier">float</span></dt><dd><p>The precision of the solution when search is used to compute the solution</p>
</dd>
<dt><a href="#id9"><span class="problematic" id="id10">bincuts_</span></a><span class="classifier">ndarray, shape(n_features, b+1)</span></dt><dd><p>Bin cuts for each input feature</p>
</dd>
<dt><a href="#id11"><span class="problematic" id="id12">train_distrib_</span></a><span class="classifier">ndarray, shape (n_bins * n_features, n_classes)</span></dt><dd><p>The PDF for each class in the training set</p>
</dd>
<dt><a href="#id13"><span class="problematic" id="id14">test_distrib_</span></a><span class="classifier">ndarray, shape (n_bins * n_features, 1) multiclass</span></dt><dd><p>The PDF for the testing bag</p>
</dd>
<dt><a href="#id15"><span class="problematic" id="id16">problem_</span></a><span class="classifier">a cvxpy Problem object</span></dt><dd><p>This attribute is set to None in the fit() method. With such model, the first time a testing bag is
predicted this attribute will contain the corresponding cvxpy Object (if such library is used, i.e in the
case of ‘L1’ and ‘HD’). For the rest testing bags, this object is passed to allow a warm start. The
solving process is faster.</p>
</dd>
<dt><a href="#id17"><span class="problematic" id="id18">mixtures_</span></a><span class="classifier">ndarray, shape (101, n_quantiles)</span></dt><dd><p>Contains the mixtures for all the prevalences in the range [0, 1] step=0.01. This speeds up the prediction
for a collection of testing bags</p>
</dd>
<dt>verbose<span class="classifier">int</span></dt><dd><p>The verbosity level</p>
</dd>
</dl>
<p>Víctor González-Castro, Rocío Alaiz-Rodríguez, and Enrique Alegre: Class Distribution Estimation based
on the Hellinger Distance. Information Sciences 218 (2013), 146–164.</p>
<p>Aykut Firat. 2016. Unified Framework for Quantification. arXiv preprint arXiv:1606.00868 (2016).</p>
<p>Dirk Tasche: Confidence intervals for class prevalences under prior probability shift. Machine Learning
and Knowledge Extraction, 1(3), (2019) 805-831.</p>
</div></blockquote>
<dl class="py method">
<dt class="sig sig-object py" id="quantificationlib.multiclass.df.DFX.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/quantificationlib/multiclass/df.html#DFX.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#quantificationlib.multiclass.df.DFX.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>This method just computes the PDFs for all the classes in the training set. The values are stored in
<a href="#id19"><span class="problematic" id="id20">train_dist_</span></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>n_features</em><em>)</em>) – Data</p></li>
<li><p><strong>y</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>)</em>) – True classes</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="quantificationlib.multiclass.df.DFX.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/quantificationlib/multiclass/df.html#DFX.predict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#quantificationlib.multiclass.df.DFX.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict the class distribution of a testing bag</p>
<p>First, the method computes the PDF for the testing bag.</p>
<p>After that, the prevalences are computed using the corresponding function according to the value of
distance attribute</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>X</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>n_features</em><em>)</em>) – Testing bag</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>prevalences</strong> – Contains the predicted prevalence for each class</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ndarray, shape(n_classes, )</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="quantificationlib.multiclass.df.DFy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">DFy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">estimator_train</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator_test</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distribution_function</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'PDF'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_bins</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bin_strategy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'equal_width'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'HD'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/quantificationlib/multiclass/df.html#DFy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#quantificationlib.multiclass.df.DFy" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="rsts_t4/quantificationlib.base.html#quantificationlib.base.UsingClassifiers" title="quantificationlib.base.UsingClassifiers"><code class="xref py py-class docutils literal notranslate"><span class="pre">UsingClassifiers</span></code></a></p>
<p>Generic Multiclass DFy method</p>
<blockquote>
<div><p>The idea is to represent the mixture of the training distribution and the testing distribution
(using CDFs/PDFs) of the predictions given by a classifier (y). The difference between both is minimized
using a distance/loss function. Originally, (González-Castro et al. 2013) propose the combination of PDF and
Hellinger Distance, but also CDF and any other distance/loss function could be used, like L1 or L2. In fact,
Forman (2005) propose to use CDF’s an a measure equivalent to L1.</p>
<p>The class has two parameters to select:</p>
<ul class="simple">
<li><p>the method used to represent the distributions (CDFs or PDFs)</p></li>
<li><p>the distance used.</p></li>
</ul>
<p>This class (as every other class based on distribution matching using classifiers) works in two different ways:</p>
<ol class="arabic simple">
<li><p>Two estimators are used to classify training examples and testing examples in order to
compute the distribution of both sets. Estimators can be already trained</p></li>
<li><p>You can directly provide the predictions for the examples in the fit/predict methods. This is useful
for synthetic/artificial experiments</p></li>
</ol>
<p>The goal in both cases is to guarantee that all methods based on distribution matching are using <strong>exactly</strong>
the same predictions when you compare this kind of quantifiers (and others that also employ an underlying
classifier, for instance, CC/PCC and AC/PAC). In the first case, estimators are only trained once and can
be shared for several quantifiers of this kind</p>
<dl>
<dt>estimator_train<span class="classifier">estimator object (default=None)</span></dt><dd><p>An estimator object implementing <cite>fit</cite> and <cite>predict_proba</cite>. It is used to classify the examples of the
training set and to compute the distribution of each class individually</p>
</dd>
<dt>estimator_test<span class="classifier">estimator object (default=None)</span></dt><dd><p>An estimator object implementing <cite>fit</cite> and <cite>predict_proba</cite>. It is used to classify the examples of the
testing set and to compute the distribution of the whole testing set</p>
</dd>
<dt>distribution_function<span class="classifier">str, (default=’PDF’)</span></dt><dd><p>Type of distribution function used. Two types are supported ‘CDF’ and ‘PDF’</p>
</dd>
<dt>n_bins<span class="classifier">int  (default=8)</span></dt><dd><p>Number of bins to compute the CDFs/PDFs</p>
</dd>
<dt>bin_strategy<span class="classifier">str (default=’norm’)</span></dt><dd><dl>
<dt>Method to compute the boundaries of the bins:</dt><dd><p>‘equal_width’: bins of equal length (it could be affected by outliers)
‘equal_count’: bins of equal counts (considering the examples of all classes)
‘binormal’: (Only for binary quantification) It is inspired on the method devised by</p>
<blockquote>
<div><p>(Tasche, 2019, Eq (A16b)). the cut points, $-infty &lt; c_1 &lt; ldots &lt; c_{b-1} &lt; infty$,
are computed as follows based on the assumption that the features follow a normal distribution:</p>
<p>$c_i =</p>
</div></blockquote>
</dd>
</dl>
</dd>
</dl>
</div></blockquote>
<p>rac{sigma^+ + sigma^{-}}{2} Phi^{-1}igg(
rac{i}{b}igg)  +
rac{mu^+ + mu^{-}}{2} ,  quad i=1,ldots,b-1$</p>
<blockquote>
<div><blockquote>
<div><p>where $Phi^{-1}$ is the quantile function of the standard normal distribution, and $mu$
and $sigma$ of the normal distribution are estimated as the average of those values for
the training examples of each class.</p>
</div></blockquote>
<dl>
<dt>‘normal’:  The idea is that each feacture follows a normal distribution. $mu$ and $sigma$ are</dt><dd><p>estimated as the weighted mean and std from the training distribution. The cut points
$-infty &lt; c_1 &lt; ldots &lt; c_{b-1} &lt; infty$ are computed as follows:</p>
<p>$c_i = sigma^ Phi^{-1}igg(</p>
</dd>
</dl>
</div></blockquote>
<p>rac{i}{b}igg)  + mu ,  quad i=1,ldots,b-1$</p>
<blockquote>
<div><dl class="simple">
<dt>distance<span class="classifier">str, representing the distance function (default=’HD’)</span></dt><dd><p>It is the name of the distance used to compute the difference between the mixture of the training
distribution and the testing distribution</p>
</dd>
<dt>tol<span class="classifier">float, (default=1e-05)</span></dt><dd><p>The precision of the solution when search is used to compute the prevalence</p>
</dd>
<dt>verbose<span class="classifier">int, optional, (default=0)</span></dt><dd><p>The verbosity level. The default value, zero, means silent mode</p>
</dd>
</dl>
<p>For some experiments both estimator_train and estimator_test could be the same</p>
<dl>
<dt><a href="#id21"><span class="problematic" id="id22">classes_</span></a><span class="classifier">ndarray, shape (n_classes, )</span></dt><dd><p>Class labels</p>
</dd>
<dt>estimator_train<span class="classifier">estimator</span></dt><dd><p>Estimator used to classify the examples of the training set</p>
</dd>
<dt>estimator_test<span class="classifier">estimator</span></dt><dd><p>Estimator used to classify the examples of the testing bag</p>
</dd>
<dt><a href="#id23"><span class="problematic" id="id24">predictions_train_</span></a><span class="classifier">ndarray, shape (n_examples, n_classes) (probabilities)</span></dt><dd><p>Predictions of the examples in the training set</p>
</dd>
<dt><a href="#id25"><span class="problematic" id="id26">predictions_test_</span></a><span class="classifier">ndarray, shape (n_examples, n_classes) (probabilities)</span></dt><dd><p>Predictions of the examples in the testing bag</p>
</dd>
<dt>needs_predictions_train<span class="classifier">bool, True</span></dt><dd><p>It is True because PDFy quantifiers need to estimate the training distribution</p>
</dd>
<dt>probabilistic_predictions<span class="classifier">bool, True</span></dt><dd><p>This means that <a href="#id27"><span class="problematic" id="id28">predictions_train_</span></a>/<a href="#id29"><span class="problematic" id="id30">predictions_test_</span></a> contain probabilistic predictions</p>
</dd>
<dt>bin_strategy<span class="classifier">str</span></dt><dd><p>Method to compute the boundaries of the bins</p>
</dd>
<dt>distance<span class="classifier">str or a distance function</span></dt><dd><p>A string with the name of the distance function (‘HD’/’L1’/’L2’) or a distance function</p>
</dd>
<dt><a href="#id31"><span class="problematic" id="id32">bincuts_</span></a><span class="classifier">ndarray, shape(n_features, b+1)</span></dt><dd><p>Bin cuts for each input feature</p>
</dd>
<dt>tol<span class="classifier">float</span></dt><dd><p>The precision of the solution when search is used to compute the solution</p>
</dd>
<dt><a href="#id33"><span class="problematic" id="id34">classes_</span></a><span class="classifier">ndarray, shape (n_classes, )</span></dt><dd><p>Class labels</p>
</dd>
<dt><a href="#id35"><span class="problematic" id="id36">y_ext_</span></a><span class="classifier">ndarray, shape(len(<a href="#id37"><span class="problematic" id="id38">predictions_train_</span></a>, 1)</span></dt><dd><p>Repmat of true labels of the training set. When CV_estimator is used with averaged_predictions=False,
<a href="#id39"><span class="problematic" id="id40">predictions_train_</span></a> will have a larger dimension (factor=n_repetitions * n_folds of the underlying CV)
than y. In other cases, <a href="#id41"><span class="problematic" id="id42">y_ext_</span></a> == y.
<a href="#id43"><span class="problematic" id="id44">y_ext_</span></a> i used in <cite>fit</cite> method whenever the true labels of the training set are needed, instead of y</p>
</dd>
<dt>distribution_function<span class="classifier">str</span></dt><dd><p>Type of distribution function used. Two types are supported ‘CDF’ and ‘PDF’</p>
</dd>
<dt>n_bins<span class="classifier">int</span></dt><dd><p>The number of bins to compute the CDFs/PDFs</p>
</dd>
<dt><a href="#id45"><span class="problematic" id="id46">train_distrib_</span></a><span class="classifier">ndarray, shape (n_bins * 1, n_classes) binary or (n_bins * <a href="#id47"><span class="problematic" id="id48">n_classes_</span></a>, n_classes) multiclass</span></dt><dd><p>The CDF/PDF for each class in the training set</p>
</dd>
<dt><a href="#id49"><span class="problematic" id="id50">test_distrib_</span></a><span class="classifier">ndarray, shape (n_bins * 1, 1) binary quantification or (n_bins * <a href="#id51"><span class="problematic" id="id52">n_classes_</span></a>, 1) multiclass q</span></dt><dd><p>The CDF/PDF for the testing bag</p>
</dd>
<dt><a href="#id53"><span class="problematic" id="id54">G_</span></a>, <a href="#id55"><span class="problematic" id="id56">C_</span></a>, <a href="#id57"><span class="problematic" id="id58">b_</span></a>: variables of different kind for defining the optimization problem</dt><dd><p>These variables are precomputed in the <cite>fit</cite> method and are used for solving the optimization problem
using <cite>quadprog.solve_qp</cite>. See <cite>compute_l2_param_train</cite> function</p>
</dd>
<dt><a href="#id59"><span class="problematic" id="id60">problem_</span></a><span class="classifier">a cvxpy Problem object</span></dt><dd><p>This attribute is set to None in the fit() method. With such model, the first time a testing bag is
predicted this attribute will contain the corresponding cvxpy Object (if such library is used, i.e in the
case of ‘L1’ and ‘HD’). For the rest testing bags, this object is passed to allow a warm start. The
solving process is faster.</p>
</dd>
<dt><a href="#id61"><span class="problematic" id="id62">mixtures_</span></a><span class="classifier">ndarray, shape (101, n_quantiles)</span></dt><dd><p>Contains the mixtures for all the prevalences in the range [0, 1] step=0.01. This speeds up the prediction
for a collection of testing bags</p>
</dd>
<dt>verbose<span class="classifier">int</span></dt><dd><p>The verbosity level</p>
</dd>
</dl>
<p>Notice that at least one between estimator_train/predictions_train and estimator_test/predictions_test
must be not None. If both are None a ValueError exception will be raised. If both are not None,
predictions_train/predictions_test are used</p>
<p>Víctor González-Castro, Rocío Alaiz-Rodríguez, and Enrique Alegre: Class Distribution Estimation based
on the Hellinger Distance. Information Sciences 218 (2013), 146–164.</p>
<p>George Forman: Counting positives accurately despite inaccurate classification. In: Proceedings of the 16th
European conference on machine learning (ECML’05), Porto, (2005) pp 564–575</p>
<p>Aykut Firat. 2016. Unified Framework for Quantification. arXiv preprint arXiv:1606.00868 (2016).</p>
<p>Dirk Tasche: Confidence intervals for class prevalences under prior probability shift. Machine Learning
and Knowledge Extraction, 1(3), (2019) 805-831.</p>
</div></blockquote>
<dl class="py method">
<dt class="sig sig-object py" id="quantificationlib.multiclass.df.DFy.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predictions_train</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/quantificationlib/multiclass/df.html#DFy.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#quantificationlib.multiclass.df.DFy.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>This method performs the following operations: 1) fits the estimators for the training set and the
testing set (if needed), and 2) computes <a href="#id63"><span class="problematic" id="id64">predictions_train_</span></a> (probabilities) if needed. Both operations are
performed by the <cite>fit</cite> method of its superclass.
After that, the method computes the pdfs for all the classes in the training set</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>n_features</em><em>)</em>) – Data</p></li>
<li><p><strong>y</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>)</em>) – True classes</p></li>
<li><p><strong>predictions_train</strong> (<em>ndarray</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>n_classes</em><em>)</em>) – Predictions of the examples in the training set</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – When estimator_train and predictions_train are both None</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="quantificationlib.multiclass.df.DFy.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predictions_test</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/quantificationlib/multiclass/df.html#DFy.predict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#quantificationlib.multiclass.df.DFy.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict the class distribution of a testing bag</p>
<p>First, <a href="#id65"><span class="problematic" id="id66">predictions_test_</span></a> are computed (if needed, when predictions_test parameter is None) by
<cite>super().predict()</cite> method.</p>
<p>After that, the method computes the PDF for the testing bag.</p>
<p>Finally, the prevalences are computed using the corresponding function according to the value of
distance attribute</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>n_features</em><em>)</em>) – Testing bag</p></li>
<li><p><strong>predictions_test</strong> (<em>ndarray</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>n_classes</em><em>) </em><em>(</em><em>default=None</em><em>)</em>) – <p>They must be probabilities (the estimator used must have a <cite>predict_proba</cite> method)</p>
<p>If predictions_test is not None they are copied on <a href="#id67"><span class="problematic" id="id68">predictions_test_</span></a> and used.
If predictions_test is None, predictions for the testing examples are computed using the <cite>predict</cite>
method of estimator_test (it must be an actual estimator)</p>
</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – When estimator_test and predictions_test are both None</p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>prevalences</strong> – Contains the predicted prevalence for each class</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>ndarray, shape(n_classes, )</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="quantificationlib.multiclass.df.HDX">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">HDX</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_bins</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bin_strategy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'equal_width'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/quantificationlib/multiclass/df.html#HDX"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#quantificationlib.multiclass.df.HDX" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="rsts_t4/quantificationlib.multiclass.df.html#quantificationlib.multiclass.df.DFX" title="quantificationlib.multiclass.df.DFX"><code class="xref py py-class docutils literal notranslate"><span class="pre">DFX</span></code></a></p>
<p>Multiclass HDX method</p>
<p>This class is a wrapper. It just uses all the inherited methods of its superclass (DFX)</p>
<p class="rubric">References</p>
<p>Víctor González-Castro, Rocío Alaiz-Rodríguez, and Enrique Alegre: Class Distribution Estimation based on
the Hellinger Distance. Information Sciences 218 (2013), 146–164.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="quantificationlib.multiclass.df.HDy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">HDy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">estimator_train</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator_test</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_bins</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bin_strategy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'equal_width'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/quantificationlib/multiclass/df.html#HDy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#quantificationlib.multiclass.df.HDy" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="rsts_t4/quantificationlib.multiclass.df.html#quantificationlib.multiclass.df.DFy" title="quantificationlib.multiclass.df.DFy"><code class="xref py py-class docutils literal notranslate"><span class="pre">DFy</span></code></a></p>
<p>Multiclass HDy method</p>
<p>This class is just a wrapper. It just uses all the inherited methods of its superclass (DFy)</p>
<p class="rubric">References</p>
<p>Víctor González-Castro, Rocío Alaiz-Rodríguez, and Enrique Alegre: Class Distribution Estimation based
on the Hellinger Distance. Information Sciences 218 (2013), 146–164.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="quantificationlib.multiclass.df.MMy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">MMy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">estimator_train</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator_test</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_bins</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bin_strategy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'equal_width'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/quantificationlib/multiclass/df.html#MMy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#quantificationlib.multiclass.df.MMy" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="rsts_t4/quantificationlib.multiclass.df.html#quantificationlib.multiclass.df.DFy" title="quantificationlib.multiclass.df.DFy"><code class="xref py py-class docutils literal notranslate"><span class="pre">DFy</span></code></a></p>
<p>Multiclass MM method</p>
<p>This class is just a wrapper. It just uses all the inherited methods of its superclass (DFy)</p>
<p class="rubric">References</p>
<p>George Forman: Counting positives accurately despite inaccurate classification. In: Proceedings of the 16th
European conference on machine learning (ECML’05), Porto, (2005) pp 564–575</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="quantificationlib.multiclass.df.compute_bincuts">
<span class="sig-name descname"><span class="pre">compute_bincuts</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_bins</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bin_strategy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'equal_width'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">att_range</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/quantificationlib/multiclass/df.html#compute_bincuts"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#quantificationlib.multiclass.df.compute_bincuts" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Compute the bincuts for calculate a histrogram with the values in X. These bincuts depends on</dt><dd><p>the bincut strategy</p>
<dl>
<dt>x<span class="classifier">array-like, shape (n_examples, )</span></dt><dd><p>Input feature</p>
</dd>
<dt>y<span class="classifier">array-like, shape (n_examples, ), (default=None)</span></dt><dd><p>True classes. It is needed when bin_strategy is ‘binormal’. In other cases, it is ignored</p>
</dd>
<dt>classes<span class="classifier">ndarray, shape (n_classes, )</span></dt><dd><p>Class labels</p>
</dd>
<dt>n_bins<span class="classifier">int, (default=8)</span></dt><dd><p>Number of bins</p>
</dd>
<dt>bin_strategy<span class="classifier">str (default=’equal_width’)</span></dt><dd><dl>
<dt>Method to compute the boundaries of the bins:</dt><dd><p>‘equal_width’: bins of equal length (it could be affected by outliers)
‘equal_count’: bins of equal counts (considering the examples of all classes)
‘binormal’: (Only for binary quantification) It is inspired on the method devised by</p>
<blockquote>
<div><p>(Tasche, 2019, Eq (A16b)). the cut points, $-infty &lt; c_1 &lt; ldots &lt; c_{b-1} &lt; infty$,
are computed as follows based on the assumption that the features follow a normal distribution:</p>
<p>$c_i =</p>
</div></blockquote>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
<p>rac{sigma^+ + sigma^{-}}{2} Phi^{-1}igg(
rac{i}{b}igg)  +
rac{mu^+ + mu^{-}}{2} ,  quad i=1,ldots,b-1$</p>
<blockquote>
<div><blockquote>
<div><p>where $Phi^{-1}$ is the quantile function of the standard normal distribution, and $mu$
and $sigma$ of the normal distribution are estimated as the average of those values for
the training examples of each class.</p>
</div></blockquote>
<dl>
<dt>‘normal’:  The idea is that each feacture follows a normal distribution. $mu$ and $sigma$ are</dt><dd><p>estimated as the weighted mean and std from the training distribution. The cut points
$-infty &lt; c_1 &lt; ldots &lt; c_{b-1} &lt; infty$ are computed as follows:</p>
<p>$c_i = sigma^ Phi^{-1}igg(</p>
</dd>
</dl>
</div></blockquote>
<p>rac{i}{b}igg)  + mu ,  quad i=1,ldots,b-1$</p>
<blockquote>
<div><dl class="simple">
<dt>att_range: array-like, (2,1)</dt><dd><p>Min and Max possible values of the input feature x. These values might not coincide with the actual Min and
Max values of vector x. For instance, in the case of x represents a set of probabilistic predictions, these
values will be 0 and 1</p>
</dd>
</dl>
<dl class="simple">
<dt>bincuts: float, shape (n_bins +1 , )</dt><dd><p>Bin cuts for input feature x</p>
</dd>
</dl>
</div></blockquote>
</dd></dl>

</section>
<section id="module-quantificationlib.multiclass.em">
<span id="quantificationlib-multiclass-em-module"></span><h2>quantificationlib.multiclass.em module<a class="headerlink" href="#module-quantificationlib.multiclass.em" title="Permalink to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="quantificationlib.multiclass.em.EM">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">EM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">estimator_train</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator_test</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/quantificationlib/multiclass/em.html#EM"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#quantificationlib.multiclass.em.EM" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="rsts_t4/quantificationlib.base.html#quantificationlib.base.UsingClassifiers" title="quantificationlib.base.UsingClassifiers"><code class="xref py py-class docutils literal notranslate"><span class="pre">UsingClassifiers</span></code></a></p>
<p>EM method</p>
<p>The main idea of this method is to estimate the prevalences applying EM algorithm.</p>
<p>This class (as every other class based on using a classifier) works in two different ways:</p>
<ol class="arabic simple">
<li><p>Two estimators are used to classify training examples and testing examples in order to
compute the distribution of both sets. Estimators can be already trained</p></li>
<li><p>You can directly provide the predictions for the examples in the fit/predict methods. This is useful
for synthetic/artificial experiments</p></li>
</ol>
<p>The goal in both cases is to guarantee that all methods based on classifier are using <strong>exactly</strong>
the same predictions when you compare this kind of quantifiers. In the first case, estimators are only trained
once and can be shared for several quantifiers of this kind</p>
<p>Theoretically, it also works for multiclass quantification.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>estimator_train</strong> (<em>estimator object</em><em> (</em><em>default=None</em><em>)</em>) – An estimator object implementing <cite>fit</cite> and <cite>predict_proba</cite>. It is used to classify the examples of the
training set and to compute the distribution of each class individually</p></li>
<li><p><strong>estimator_test</strong> (<em>estimator object</em><em> (</em><em>default=None</em><em>)</em>) – An estimator object implementing <cite>fit</cite> and <cite>predict_proba</cite>. It is used to classify the examples of the
testing set and to compute the distribution of the whole testing set</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>(</em><em>default=0</em><em>)</em>) – The verbosity level. The default value, zero, means silent mode</p></li>
<li><p><strong>epsilon</strong> (<em>float</em><em>, </em><em>(</em><em>default=1e-04</em><em>)</em>) – EM algorithm (its loop) stops when the difference between the prevalences of two
consecutive iterations is lower than epsilon</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em> (</em><em>default=1000</em><em>)</em>) – The maximum number of iterations for the loop of the EM algorithm</p></li>
<li><p><strong>same</strong> (<em>For some experiments both estimator_train and estimator_test could be the</em>) – </p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.em.EM.estimator_train">
<span class="sig-name descname"><span class="pre">estimator_train</span></span><a class="headerlink" href="#quantificationlib.multiclass.em.EM.estimator_train" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimator used to classify the examples of the training set</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>estimator</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.em.EM.estimator_test">
<span class="sig-name descname"><span class="pre">estimator_test</span></span><a class="headerlink" href="#quantificationlib.multiclass.em.EM.estimator_test" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimator used to classify the examples of the testing bag</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>estimator</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.em.EM.predictions_train_">
<span class="sig-name descname"><span class="pre">predictions_train_</span></span><a class="headerlink" href="#quantificationlib.multiclass.em.EM.predictions_train_" title="Permalink to this definition">¶</a></dt>
<dd><p>Predictions of the examples in the training set</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ndarray, shape (n_examples, n_classes) (probabilities)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.em.EM.predictions_test_">
<span class="sig-name descname"><span class="pre">predictions_test_</span></span><a class="headerlink" href="#quantificationlib.multiclass.em.EM.predictions_test_" title="Permalink to this definition">¶</a></dt>
<dd><p>Predictions of the examples in the testing bag</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ndarray, shape (n_examples, n_classes) (probabilities)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.em.EM.needs_predictions_train">
<span class="sig-name descname"><span class="pre">needs_predictions_train</span></span><a class="headerlink" href="#quantificationlib.multiclass.em.EM.needs_predictions_train" title="Permalink to this definition">¶</a></dt>
<dd><p>It is True because SORD quantifiers need to estimate the training distribution</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>bool, True</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.em.EM.probabilistic_predictions">
<span class="sig-name descname"><span class="pre">probabilistic_predictions</span></span><a class="headerlink" href="#quantificationlib.multiclass.em.EM.probabilistic_predictions" title="Permalink to this definition">¶</a></dt>
<dd><p>This means that <a href="#id69"><span class="problematic" id="id70">predictions_train_</span></a>/<a href="#id71"><span class="problematic" id="id72">predictions_test_</span></a> contain probabilistic predictions</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>bool, True</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.em.EM.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#quantificationlib.multiclass.em.EM.classes_" title="Permalink to this definition">¶</a></dt>
<dd><p>Class labels</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ndarray, shape (n_classes, )</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.em.EM.y_ext_">
<span class="sig-name descname"><span class="pre">y_ext_</span></span><a class="headerlink" href="#quantificationlib.multiclass.em.EM.y_ext_" title="Permalink to this definition">¶</a></dt>
<dd><p>Repmat of true labels of the training set. When CV_estimator is used with averaged_predictions=False,
<a href="#id73"><span class="problematic" id="id74">predictions_train_</span></a> will have a larger dimension (factor=n_repetitions * n_folds of the underlying CV)
than y. In other cases, <a href="#id75"><span class="problematic" id="id76">y_ext_</span></a> == y.
<a href="#id77"><span class="problematic" id="id78">y_ext_</span></a> i used in <cite>fit</cite>/<cite>predict</cite> method whenever the true labels of the training set are needed,
instead of y</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ndarray, shape(len(<a href="#id79"><span class="problematic" id="id80">predictions_train_</span></a>, 1)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.em.EM.verbose">
<span class="sig-name descname"><span class="pre">verbose</span></span><a class="headerlink" href="#quantificationlib.multiclass.em.EM.verbose" title="Permalink to this definition">¶</a></dt>
<dd><p>The verbosity level</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.em.EM.epsilon_">
<span class="sig-name descname"><span class="pre">epsilon_</span></span><a class="headerlink" href="#quantificationlib.multiclass.em.EM.epsilon_" title="Permalink to this definition">¶</a></dt>
<dd><p>EM algorithm (its loop) stops when the mean absolute error between the prevalences of two
consecutive iterations is lower than epsilon</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.em.EM.max_iter_">
<span class="sig-name descname"><span class="pre">max_iter_</span></span><a class="headerlink" href="#quantificationlib.multiclass.em.EM.max_iter_" title="Permalink to this definition">¶</a></dt>
<dd><p>The maximum number of iterations for the loop of the EM algorithm</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.em.EM.prevalences_train_">
<span class="sig-name descname"><span class="pre">prevalences_train_</span></span><a class="headerlink" href="#quantificationlib.multiclass.em.EM.prevalences_train_" title="Permalink to this definition">¶</a></dt>
<dd><p>Prevalence of each class in the training dataset</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ndarray, shape (n_classes, )</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Notes</p>
<p>Notice that at least one between estimator_train/predictions_train and estimator_test/predictions_test
must be not None. If both are None a ValueError exception will be raised. If both are not None,
predictions_train/predictions_test are used</p>
<p class="rubric">References</p>
<p>Marco Saerens, Patrice Latinne, Christine Decaestecker: Adjusting the outputs of a classifier to new a priori
probabilities: a simple procedure. Neural computation, 14(1) (2002), 21-41.</p>
<dl class="py method">
<dt class="sig sig-object py" id="quantificationlib.multiclass.em.EM.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predictions_train</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/quantificationlib/multiclass/em.html#EM.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#quantificationlib.multiclass.em.EM.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>This method performs the following operations: 1) fits the estimators for the training set and the
testing set (if needed), and 2) computes <a href="#id81"><span class="problematic" id="id82">predictions_train_</span></a> (probabilities) if needed. Both operations are
performed by the <cite>fit</cite> method of its superclass.</p>
<p>After that, the method just computes the prevalences in the training set</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>n_features</em><em>)</em>) – Data</p></li>
<li><p><strong>y</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>)</em>) – True classes</p></li>
<li><p><strong>predictions_train</strong> (<em>ndarray</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>n_classes</em><em>)</em>) – Predictions of the examples in the training set</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – When estimator_train and predictions_train are at the same time None or not None</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="quantificationlib.multiclass.em.EM.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predictions_test</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/quantificationlib/multiclass/em.html#EM.predict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#quantificationlib.multiclass.em.EM.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict the class distribution of a testing bag</p>
<p>First, <a href="#id83"><span class="problematic" id="id84">predictions_test_</span></a> are computed (if needed, when predictions_test parameter is None) by
<cite>super().predict()</cite> method.</p>
<p>After that, the method applies EM algorithm. In the E step the new posterior are computed based on the
estimated prevalences</p>
<blockquote>
<div><p>P_tst(y|x) = P_tst(y)/P_tr(y) P_tr(y|x)</p>
</div></blockquote>
<p>and are normalized to sum 1
Then, in the M step new prevalences are computed as the mean of the posteriors obtained in the E step</p>
<p>The loop stops when the max number of iterations is reached or when the difference between the
prevalences of two consecutive iterations is lower than epsilon</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>n_features</em><em>)</em>) – Testing bag</p></li>
<li><p><strong>predictions_test</strong> (<em>ndarray</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>n_classes</em><em>) </em><em>(</em><em>default=None</em><em>)</em>) – <p>They must be probabilities (the estimator used must have a <cite>predict_proba</cite> method)</p>
<p>If estimator_test is None then predictions_test can not be None.
If predictions_test is None, predictions for the testing examples are computed using the <cite>predict_proba</cite>
method of estimator_test (it must be an actual estimator)</p>
</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – When estimator_test and predictions_test are at the same time None or not None</p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>prevalences</strong> – Contains the predicted prevalence for each class</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>ndarray, shape(n_classes, )</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-quantificationlib.multiclass.energy">
<span id="quantificationlib-multiclass-energy-module"></span><h2>quantificationlib.multiclass.energy module<a class="headerlink" href="#module-quantificationlib.multiclass.energy" title="Permalink to this heading">¶</a></h2>
<p>Multiclass versions for quantifiers based on the Energy Distance</p>
<dl class="py class">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.CvMy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">CvMy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">estimator_train=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator_test=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distance=&lt;function</span> <span class="pre">manhattan_distances&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose=0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/quantificationlib/multiclass/energy.html#CvMy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#quantificationlib.multiclass.energy.CvMy" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="rsts_t4/quantificationlib.base.html#quantificationlib.base.UsingClassifiers" title="quantificationlib.base.UsingClassifiers"><code class="xref py py-class docutils literal notranslate"><span class="pre">UsingClassifiers</span></code></a></p>
<p>Multiclass CvMy method</p>
<p>As described in (Castaño et al 2019), the predicted prevalences can be analytically calculated solving an
optimization problem (with quadprog.solve_qp in this library). All ED-based methods share several functions
in distribution_matching.utils. These functions are used to compute the elements of the optimization
problem (<cite>compute_ed_param_train</cite>, <cite>compute_ed_param_test</cite>) and to solve the optimization problem (<cite>solve_ed</cite>)</p>
<p>This class (as every other class based on distribution matching using classifiers) works in two different ways:</p>
<ol class="arabic simple">
<li><p>Two estimators are used to classify training examples and testing examples in order to
compute the distribution of both sets. Estimators can be already trained</p></li>
<li><p>You can directly provide the predictions for the examples in the fit/predict methods. This is useful
for synthetic/artificial experiments</p></li>
</ol>
<p>The idea in both cases is to guarantee that all methods based on distribution matching are using <strong>exactly</strong>
the same predictions when you compare this kind of quantifiers (and others that also employ an underlying
classifier, for instance, CC/PCC and AC/PAC). In the first case, estimators are only trained once and can
be shared for several quantifiers of this kind</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>estimator_train</strong> (<em>estimator object</em><em> (</em><em>default=None</em><em>)</em>) – An estimator object implementing <cite>fit</cite> and <cite>predict_proba</cite>. It is used to classify the examples of the
training set and to compute the distribution of each class individually</p></li>
<li><p><strong>estimator_test</strong> (<em>estimator object</em><em> (</em><em>default=None</em><em>)</em>) – An estimator object implementing <cite>fit</cite> and <cite>predict_proba</cite>. It is used to classify the examples of the
testing set and to compute the distribution of the whole testing set</p></li>
<li><p><strong>distance</strong> (<em>distance function</em><em> (</em><em>default=manhattan_distances</em><em>)</em>) – It is the function used to compute the distance between every pair of examples</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>(</em><em>default=0</em><em>)</em>) – The verbosity level. The default value, zero, means silent mode</p></li>
<li><p><strong>same</strong> (<em>For some experiments both estimator_train and estimator_test could be the</em>) – </p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.CvMy.estimator_train">
<span class="sig-name descname"><span class="pre">estimator_train</span></span><a class="headerlink" href="#quantificationlib.multiclass.energy.CvMy.estimator_train" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimator used to classify the examples of the training set</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>estimator</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.CvMy.estimator_test">
<span class="sig-name descname"><span class="pre">estimator_test</span></span><a class="headerlink" href="#quantificationlib.multiclass.energy.CvMy.estimator_test" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimator used to classify the examples of the testing bag</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>estimator</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.CvMy.predictions_train_">
<span class="sig-name descname"><span class="pre">predictions_train_</span></span><a class="headerlink" href="#quantificationlib.multiclass.energy.CvMy.predictions_train_" title="Permalink to this definition">¶</a></dt>
<dd><p>Predictions of the examples in the training set</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ndarray, shape (n_examples, n_classes) (probabilities)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.CvMy.predictions_test_">
<span class="sig-name descname"><span class="pre">predictions_test_</span></span><a class="headerlink" href="#quantificationlib.multiclass.energy.CvMy.predictions_test_" title="Permalink to this definition">¶</a></dt>
<dd><p>Predictions of the examples in the testing bag</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ndarray, shape (n_examples, n_classes) (probabilities)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.CvMy.needs_predictions_train">
<span class="sig-name descname"><span class="pre">needs_predictions_train</span></span><a class="headerlink" href="#quantificationlib.multiclass.energy.CvMy.needs_predictions_train" title="Permalink to this definition">¶</a></dt>
<dd><p>It is True because CvMy quantifiers need to estimate the training distribution</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>bool, True</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.CvMy.probabilistic_predictions">
<span class="sig-name descname"><span class="pre">probabilistic_predictions</span></span><a class="headerlink" href="#quantificationlib.multiclass.energy.CvMy.probabilistic_predictions" title="Permalink to this definition">¶</a></dt>
<dd><p>This means that <a href="#id85"><span class="problematic" id="id86">predictions_train_</span></a>/<a href="#id87"><span class="problematic" id="id88">predictions_test_</span></a> contain probabilistic predictions</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>bool, True</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.CvMy.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#quantificationlib.multiclass.energy.CvMy.classes_" title="Permalink to this definition">¶</a></dt>
<dd><p>Class labels</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ndarray, shape (n_classes, )</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.CvMy.y_ext_">
<span class="sig-name descname"><span class="pre">y_ext_</span></span><a class="headerlink" href="#quantificationlib.multiclass.energy.CvMy.y_ext_" title="Permalink to this definition">¶</a></dt>
<dd><p>Repmat of true labels of the training set. When CV_estimator is used with averaged_predictions=False,
<a href="#id89"><span class="problematic" id="id90">predictions_train_</span></a> will have a larger dimension (factor=n_repetitions * n_folds of the underlying CV)
than y. In other cases, <a href="#id91"><span class="problematic" id="id92">y_ext_</span></a> == y.
<a href="#id93"><span class="problematic" id="id94">y_ext_</span></a> i used in <cite>predict</cite> method whenever the true labels of the training set are needed, instead of y</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ndarray, shape(len(<a href="#id95"><span class="problematic" id="id96">predictions_train_</span></a>, 1)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.CvMy.distance">
<span class="sig-name descname"><span class="pre">distance</span></span><a class="headerlink" href="#quantificationlib.multiclass.energy.CvMy.distance" title="Permalink to this definition">¶</a></dt>
<dd><p>Function used to compute the distance between every pair of examples</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>distance function</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.CvMy.train_n_cls_i_">
<span class="sig-name descname"><span class="pre">train_n_cls_i_</span></span><a class="headerlink" href="#quantificationlib.multiclass.energy.CvMy.train_n_cls_i_" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of the examples of each class in the training set. Used to compute average distances</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ndarray, shape(n_classes, 1)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.CvMy.train_distrib_">
<span class="sig-name descname"><span class="pre">train_distrib_</span></span><a class="headerlink" href="#quantificationlib.multiclass.energy.CvMy.train_distrib_" title="Permalink to this definition">¶</a></dt>
<dd><p>Each key has associated a ndarray with the predictions, shape (train_n_cls_i_[i], 1) (binary quantification
problems) or (train_n_cls_i_[i], n_classes) (multiclass quantification problems)</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Dict, the keys are the labels of the classes (<a href="#id97"><span class="problematic" id="id98">classes_</span></a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.CvMy.test_distrib_">
<span class="sig-name descname"><span class="pre">test_distrib_</span></span><a class="headerlink" href="#quantificationlib.multiclass.energy.CvMy.test_distrib_" title="Permalink to this definition">¶</a></dt>
<dd><p>The distribution of the test distribution</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ndarray, shape(n_examples, )</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.CvMy.K_">
<span class="sig-name descname"><span class="pre">K_</span></span><a class="headerlink" href="#quantificationlib.multiclass.energy.CvMy.K_" title="Permalink to this definition">¶</a></dt>
<dd><p>Average distance between the examples in the training set of each pair of classes</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ndarray, shape (n_classes, n_classes)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">G_,</span> <span class="pre">C_,</span> <span class="pre">b_</span></span></dt>
<dd><p>These variables are precomputed in the <cite>fit</cite> method and are used for solving the optimization problem
using <cite>quadprog.solve_qp</cite>. See <cite>compute_ed_param_train</cite> function</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>variables of different kind for definining the optimization problem</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.CvMy.a_">
<span class="sig-name descname"><span class="pre">a_</span></span><a class="headerlink" href="#quantificationlib.multiclass.energy.CvMy.a_" title="Permalink to this definition">¶</a></dt>
<dd><p>This one is computed in the <cite>predict</cite> method, just before solving the optimization problem</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>another variable of the optimization problem</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.CvMy.verbose">
<span class="sig-name descname"><span class="pre">verbose</span></span><a class="headerlink" href="#quantificationlib.multiclass.energy.CvMy.verbose" title="Permalink to this definition">¶</a></dt>
<dd><p>The verbosity level</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Notes</p>
<p>Notice that at least one between estimator_train/predictions_train and estimator_test/predictions_test
must be not None. If both are None a ValueError exception will be raised. If both are not None,
predictions_train/predictions_test are used</p>
<p class="rubric">References</p>
<p>Alberto Castaño, Laura Morán-Fernández, Jaime Alonso, Verónica Bolón-Canedo, Amparo Alonso-Betanzos,
Juan José del Coz: An analysis of quantification methods based on matching distributions</p>
<dl class="py method">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.CvMy.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predictions_train</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/quantificationlib/multiclass/energy.html#CvMy.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#quantificationlib.multiclass.energy.CvMy.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>This method performs the following operations: 1) fits the estimators for the training set and the
testing set (if needed), and 2) computes <a href="#id99"><span class="problematic" id="id100">predictions_train_</span></a> (probabilities) if needed. Both operations are
performed by the <cite>fit</cite> method of its superclass.
After that, the method stores the true classes in <a href="#id101"><span class="problematic" id="id102">y_train_</span></a> attribute.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>n_features</em><em>)</em>) – Data</p></li>
<li><p><strong>y</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>)</em>) – True classes</p></li>
<li><p><strong>predictions_train</strong> (<em>ndarray</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>n_classes</em><em>) </em><em>(</em><em>probabilities</em><em>)</em>) – Predictions of the examples in the training set</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – When estimator_train and predictions_train are both None</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.CvMy.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predictions_test</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/quantificationlib/multiclass/energy.html#CvMy.predict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#quantificationlib.multiclass.energy.CvMy.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict the class distribution of a testing bag</p>
<p>First, <a href="#id103"><span class="problematic" id="id104">predictions_test_</span></a> are computed (if needed, when predictions_test parameter is None) by
<cite>super().predict()</cite> method.</p>
<p>Then, the method computes all the elements of the optimization problem after computing the combined
ranking of the predictions for the training examples and the testing examples using <cite>rankdata</cite> function</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>n_features</em><em>)</em>) – Testing bag</p></li>
<li><p><strong>predictions_test</strong> (<em>ndarray</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>n_classes</em><em>) </em><em>(</em><em>default=None</em><em>)</em>) – <p>They must be probabilities (the estimator used must have a <cite>predict_proba</cite> method)</p>
<p>If predictions_test is not None they are copied on <a href="#id105"><span class="problematic" id="id106">predictions_test_</span></a> and used.
If predictions_test is None, predictions for the testing examples are computed using the <cite>predict</cite>
method of estimator_test (it must be an actual estimator)</p>
</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – When estimator_test and predictions_test are at the same time None or not None</p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>prevalences</strong> – Contains the predicted prevalence for each class</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>ndarray, shape(n_classes, )</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.EDX">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">EDX</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">distance=&lt;function</span> <span class="pre">euclidean_distances&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose=0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/quantificationlib/multiclass/energy.html#EDX"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#quantificationlib.multiclass.energy.EDX" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="rsts_t4/quantificationlib.base.html#quantificationlib.base.WithoutClassifiers" title="quantificationlib.base.WithoutClassifiers"><code class="xref py py-class docutils literal notranslate"><span class="pre">WithoutClassifiers</span></code></a></p>
<p>Multiclass EDX method</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>distance</strong> (<em>distance function</em><em> (</em><em>default=euclidean_distances</em><em>)</em>) – It is the function used to compute the distance between every pair of examples</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>(</em><em>default=0</em><em>)</em>) – The verbosity level. The default value, zero, means silent mode</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.EDX.distance_">
<span class="sig-name descname"><span class="pre">distance_</span></span><a class="headerlink" href="#quantificationlib.multiclass.energy.EDX.distance_" title="Permalink to this definition">¶</a></dt>
<dd><p>The distance fuction used for computing the distance between every pair of examples</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>distance function</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.EDX.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#quantificationlib.multiclass.energy.EDX.classes_" title="Permalink to this definition">¶</a></dt>
<dd><p>Class labels</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ndarray, shape (n_classes, )</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.EDX.train_n_cls_i_">
<span class="sig-name descname"><span class="pre">train_n_cls_i_</span></span><a class="headerlink" href="#quantificationlib.multiclass.energy.EDX.train_n_cls_i_" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of the examples of each class in the training set. Used to compute average distances</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ndarray, shape(n_classes, 1)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.EDX.train_distrib_">
<span class="sig-name descname"><span class="pre">train_distrib_</span></span><a class="headerlink" href="#quantificationlib.multiclass.energy.EDX.train_distrib_" title="Permalink to this definition">¶</a></dt>
<dd><p>Each key has associated a ndarray with the predictions, shape (train_n_cls_i_[i], 1) (binary quantification
problems) or (train_n_cls_i_[i], n_classes) (multiclass quantification problems)</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Dict, the keys are the labels of the classes (<a href="#id107"><span class="problematic" id="id108">classes_</span></a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.EDX.K_">
<span class="sig-name descname"><span class="pre">K_</span></span><a class="headerlink" href="#quantificationlib.multiclass.energy.EDX.K_" title="Permalink to this definition">¶</a></dt>
<dd><p>Average distance between the examples in the training set of each pair of classes</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ndarray, shape (n_classes, n_classes)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">G_,</span> <span class="pre">C_,</span> <span class="pre">b_</span></span></dt>
<dd><p>These variables are precomputed in the <cite>fit</cite> method and are used for solving the optimization problem
using <cite>quadprog.solve_qp</cite>. See <cite>compute_ed_param_train</cite> function</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>variables of different kind for definining the optimization problem</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.EDX.verbose">
<span class="sig-name descname"><span class="pre">verbose</span></span><a class="headerlink" href="#quantificationlib.multiclass.energy.EDX.verbose" title="Permalink to this definition">¶</a></dt>
<dd><p>The verbosity level</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">References</p>
<p>Hideko Kawakubo, Marthinus Christoffel Du Plessis, and Masashi Sugiyama. 2016. Computationally efficient
class-prior estimation under class balance change using energy distance. Transactions on Information
and Systems 99, 1 (2016), 176–186.</p>
<dl class="py method">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.EDX.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/quantificationlib/multiclass/energy.html#EDX.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#quantificationlib.multiclass.energy.EDX.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>This method computes all the elements of the optimization that involve just the training data:
<a href="#id109"><span class="problematic" id="id110">K_</span></a>, <a href="#id111"><span class="problematic" id="id112">G_</span></a>, <a href="#id113"><span class="problematic" id="id114">C_</span></a> and <a href="#id115"><span class="problematic" id="id116">b_</span></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>n_features</em><em>)</em>) – Data</p></li>
<li><p><strong>y</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>)</em>) – True classes</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.EDX.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/quantificationlib/multiclass/energy.html#EDX.predict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#quantificationlib.multiclass.energy.EDX.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict the class distribution of a testing bag</p>
<p>This method computes a, the only element of the optimization problem that needs the testing
data. Then, it solves the optimization problem using <cite>quadprog.solve_qp</cite> in <cite>solve_ed</cite> function</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>X</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>n_features</em><em>)</em>) – Testing bag</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>prevalences</strong> – Contains the predicted prevalence for each class</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ndarray, shape(n_classes, )</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.EDy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">EDy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">estimator_train=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator_test=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distance=&lt;function</span> <span class="pre">manhattan_distances&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose=0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/quantificationlib/multiclass/energy.html#EDy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#quantificationlib.multiclass.energy.EDy" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="rsts_t4/quantificationlib.base.html#quantificationlib.base.UsingClassifiers" title="quantificationlib.base.UsingClassifiers"><code class="xref py py-class docutils literal notranslate"><span class="pre">UsingClassifiers</span></code></a></p>
<p>Multiclass EDy method</p>
<p>As described in (Castaño et al 2019), the predicted prevalences can be analytically calculated solving an
optimization problem (with quadprog.solve_qp in this library). All ED-based methods share several functions
in distribution_matching.utils. These functions are used to compute the elements of the optimization
problem (<cite>compute_ed_param_train</cite>, <cite>compute_ed_param_test</cite>) and to solve the optimization problem (<cite>solve_ed</cite>)</p>
<p>This class (as every other class based on distribution matching using classifiers) works in two different ways:</p>
<ol class="arabic simple">
<li><p>Two estimators are used to classify training examples and testing examples in order to
compute the distribution of both sets. Estimators can be already trained</p></li>
<li><p>You can directly provide the predictions for the examples in the fit/predict methods. This is useful
for synthetic/artificial experiments</p></li>
</ol>
<p>The idea in both cases is to guarantee that all methods based on distribution matching are using <strong>exactly</strong>
the same predictions when you compare this kind of quantifiers (and others that also employ an underlying
classifier, for instance, CC/PCC and AC/PAC). In the first case, estimators are only trained once and can
be shared for several quantifiers of this kind</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>estimator_train</strong> (<em>estimator object</em><em> (</em><em>default=None</em><em>)</em>) – An estimator object implementing <cite>fit</cite> and <cite>predict_proba</cite>. It is used to classify the examples of the
training set and to compute the distribution of each class individually</p></li>
<li><p><strong>estimator_test</strong> (<em>estimator object</em><em> (</em><em>default=None</em><em>)</em>) – An estimator object implementing <cite>fit</cite> and <cite>predict_proba</cite>. It is used to classify the examples of the
testing set and to compute the distribution of the whole testing set</p></li>
<li><p><strong>distance</strong> (<em>distance function</em><em> (</em><em>default=manhattan_distances</em><em>)</em>) – It is the function used to compute the distance between every pair of examples</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>(</em><em>default=0</em><em>)</em>) – The verbosity level. The default value, zero, means silent mode</p></li>
<li><p><strong>same</strong> (<em>For some experiments both estimator_train and estimator_test could be the</em>) – </p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.EDy.estimator_train">
<span class="sig-name descname"><span class="pre">estimator_train</span></span><a class="headerlink" href="#quantificationlib.multiclass.energy.EDy.estimator_train" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimator used to classify the examples of the training set</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>estimator</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.EDy.estimator_test">
<span class="sig-name descname"><span class="pre">estimator_test</span></span><a class="headerlink" href="#quantificationlib.multiclass.energy.EDy.estimator_test" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimator used to classify the examples of the testing bag</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>estimator</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.EDy.predictions_train_">
<span class="sig-name descname"><span class="pre">predictions_train_</span></span><a class="headerlink" href="#quantificationlib.multiclass.energy.EDy.predictions_train_" title="Permalink to this definition">¶</a></dt>
<dd><p>Predictions of the examples in the training set</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ndarray, shape (n_examples, n_classes) (probabilities)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.EDy.predictions_test_">
<span class="sig-name descname"><span class="pre">predictions_test_</span></span><a class="headerlink" href="#quantificationlib.multiclass.energy.EDy.predictions_test_" title="Permalink to this definition">¶</a></dt>
<dd><p>Predictions of the examples in the testing bag</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ndarray, shape (n_examples, n_classes) (probabilities)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.EDy.needs_predictions_train">
<span class="sig-name descname"><span class="pre">needs_predictions_train</span></span><a class="headerlink" href="#quantificationlib.multiclass.energy.EDy.needs_predictions_train" title="Permalink to this definition">¶</a></dt>
<dd><p>It is True because EDy quantifiers need to estimate the training distribution</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>bool, True</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.EDy.probabilistic_predictions">
<span class="sig-name descname"><span class="pre">probabilistic_predictions</span></span><a class="headerlink" href="#quantificationlib.multiclass.energy.EDy.probabilistic_predictions" title="Permalink to this definition">¶</a></dt>
<dd><p>This means that <a href="#id117"><span class="problematic" id="id118">predictions_train_</span></a>/<a href="#id119"><span class="problematic" id="id120">predictions_test_</span></a> contain probabilistic predictions</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>bool, True</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.EDy.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#quantificationlib.multiclass.energy.EDy.classes_" title="Permalink to this definition">¶</a></dt>
<dd><p>Class labels</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ndarray, shape (n_classes, )</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.EDy.y_ext_">
<span class="sig-name descname"><span class="pre">y_ext_</span></span><a class="headerlink" href="#quantificationlib.multiclass.energy.EDy.y_ext_" title="Permalink to this definition">¶</a></dt>
<dd><p>Repmat of true labels of the training set. When CV_estimator is used with averaged_predictions=False,
<a href="#id121"><span class="problematic" id="id122">predictions_train_</span></a> will have a larger dimension (factor=n_repetitions * n_folds of the underlying CV)
than y. In other cases, <a href="#id123"><span class="problematic" id="id124">y_ext_</span></a> == y.
<a href="#id125"><span class="problematic" id="id126">y_ext_</span></a> i used in <cite>fit</cite> method whenever the true labels of the training set are needed, instead of y</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ndarray, shape(len(<a href="#id127"><span class="problematic" id="id128">predictions_train_</span></a>, 1)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.EDy.train_n_cls_i_">
<span class="sig-name descname"><span class="pre">train_n_cls_i_</span></span><a class="headerlink" href="#quantificationlib.multiclass.energy.EDy.train_n_cls_i_" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of the examples of each class in the training set. Used to compute average distances</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ndarray, shape(n_classes, 1)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.EDy.train_distrib_">
<span class="sig-name descname"><span class="pre">train_distrib_</span></span><a class="headerlink" href="#quantificationlib.multiclass.energy.EDy.train_distrib_" title="Permalink to this definition">¶</a></dt>
<dd><p>Each key has associated a ndarray with the predictions, shape (train_n_cls_i_[i], 1) (binary quantification
problems) or (train_n_cls_i_[i], n_classes) (multiclass quantification problems)</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Dict, the keys are the labels of the classes (<a href="#id129"><span class="problematic" id="id130">classes_</span></a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.EDy.K_">
<span class="sig-name descname"><span class="pre">K_</span></span><a class="headerlink" href="#quantificationlib.multiclass.energy.EDy.K_" title="Permalink to this definition">¶</a></dt>
<dd><p>Average distance between the examples in the training set of each pair of classes</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ndarray, shape (n_classes, n_classes)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">G_,</span> <span class="pre">C_,</span> <span class="pre">b_</span></span></dt>
<dd><p>These variables are precomputed in the <cite>fit</cite> method and are used for solving the optimization problem
using <cite>quadprog.solve_qp</cite>. See <cite>compute_ed_param_train</cite> function</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>variables of different kind for definining the optimization problem</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.EDy.a_">
<span class="sig-name descname"><span class="pre">a_</span></span><a class="headerlink" href="#quantificationlib.multiclass.energy.EDy.a_" title="Permalink to this definition">¶</a></dt>
<dd><p>This one is computed in the <cite>predict</cite> method, just before solving the optimization problem</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>another variable of the optimization problem</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.EDy.verbose">
<span class="sig-name descname"><span class="pre">verbose</span></span><a class="headerlink" href="#quantificationlib.multiclass.energy.EDy.verbose" title="Permalink to this definition">¶</a></dt>
<dd><p>The verbosity level</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Notes</p>
<p>Notice that at least one between estimator_train/predictions_train and estimator_test/predictions_test
must be not None. If both are None a ValueError exception will be raised. If both are not None,
predictions_train/predictions_test are used</p>
<p class="rubric">References</p>
<p>Alberto Castaño, Laura Morán-Fernández, Jaime Alonso, Verónica Bolón-Canedo, Amparo Alonso-Betanzos,
Juan José del Coz: An analysis of quantification methods based on matching distributions</p>
<dl class="py method">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.EDy.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predictions_train</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/quantificationlib/multiclass/energy.html#EDy.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#quantificationlib.multiclass.energy.EDy.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>This method performs the following operations: 1) fits the estimators for the training set and the
testing set (if needed), and 2) computes <a href="#id131"><span class="problematic" id="id132">predictions_train_</span></a> (probabilities) if needed. Both operations are
performed by the <cite>fit</cite> method of its superclass.
After that, the method computes all the elements of the optimization problem that involve just the
training data:
<a href="#id133"><span class="problematic" id="id134">K_</span></a>, <a href="#id135"><span class="problematic" id="id136">G_</span></a>, <a href="#id137"><span class="problematic" id="id138">C_</span></a> and <a href="#id139"><span class="problematic" id="id140">b_</span></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>n_features</em><em>)</em>) – Data</p></li>
<li><p><strong>y</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>)</em>) – True classes</p></li>
<li><p><strong>predictions_train</strong> (<em>ndarray</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>n_classes</em><em>) </em><em>(</em><em>probabilities</em><em>)</em>) – Predictions of the examples in the training set</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – When estimator_train and predictions_train are both None</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="quantificationlib.multiclass.energy.EDy.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predictions_test</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/quantificationlib/multiclass/energy.html#EDy.predict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#quantificationlib.multiclass.energy.EDy.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict the class distribution of a testing bag</p>
<p>First, <a href="#id141"><span class="problematic" id="id142">predictions_test_</span></a> are computed (if needed, when predictions_test parameter is None) by
<cite>super().predict()</cite> method.</p>
<p>After that, the method computes a, the only element of the optimization problem that needs the testing
data</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>n_features</em><em>)</em>) – Testing bag</p></li>
<li><p><strong>predictions_test</strong> (<em>ndarray</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>n_classes</em><em>) </em><em>(</em><em>default=None</em><em>)</em>) – <p>They must be probabilities (the estimator used must have a <cite>predict_proba</cite> method)</p>
<p>If predictions_test is not None they are copied on <a href="#id143"><span class="problematic" id="id144">predictions_test_</span></a> and used.
If predictions_test is None, predictions for the testing examples are computed using the <cite>predict</cite>
method of estimator_test (it must be an actual estimator)</p>
</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – When estimator_test and predictions_test are at the same time None or not None</p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>prevalences</strong> – Contains the predicted prevalence for each class</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>ndarray, shape(n_classes, )</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-quantificationlib.multiclass.friedman">
<span id="quantificationlib-multiclass-friedman-module"></span><h2>quantificationlib.multiclass.friedman module<a class="headerlink" href="#module-quantificationlib.multiclass.friedman" title="Permalink to this heading">¶</a></h2>
<p>Quantifier based on Mixture Estimation proposed by Friedman</p>
<dl class="py class">
<dt class="sig sig-object py" id="quantificationlib.multiclass.friedman.FriedmanME">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">FriedmanME</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">estimator_test</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator_train</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'L2'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/quantificationlib/multiclass/friedman.html#FriedmanME"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#quantificationlib.multiclass.friedman.FriedmanME" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="rsts_t4/quantificationlib.base.html#quantificationlib.base.UsingClassifiers" title="quantificationlib.base.UsingClassifiers"><code class="xref py py-class docutils literal notranslate"><span class="pre">UsingClassifiers</span></code></a></p>
<p>Multiclass Mixture Estimation method proposed by Friedman</p>
<p>This class works in two different ways:</p>
<ol class="arabic simple">
<li><p>Two estimators are used to classify the examples of the training set and the testing set in order to
compute the (probabilistic) confusion matrix of both sets. Estimators can be already trained</p></li>
<li><p>You can directly provide the predictions for the examples in the <cite>fit</cite>/<a href="#id1"><span class="problematic" id="id2">`</span></a>predict methods. This is useful
for synthetic/artificial experiments</p></li>
</ol>
<p>The idea in both cases is to guarantee that all methods based on distribution matching are using <strong>exactly</strong>
the same predictions when you compare this kind of quantifiers (and others that also employ an underlying
classifier, for instance, CC/PCC). In the first case, estimators are only trained once and can be shared
for several quantifiers of this kind</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>estimator_train</strong> (<em>estimator object</em><em> (</em><em>default=None</em><em>)</em>) – An estimator object implementing <cite>fit</cite> and <cite>predict_proba</cite>. It is used to classify the examples of the
training set and to compute the confusion matrix</p></li>
<li><p><strong>estimator_test</strong> (<em>estimator object</em><em> (</em><em>default=None</em><em>)</em>) – An estimator object implementing <cite>fit</cite> and <cite>predict_proba</cite>. It is used to classify the examples of the
testing set and to obtain the confusion matrix of the testing set</p></li>
<li><p><strong>distance</strong> (<em>str</em><em>, </em><em>representing the distance function</em><em> (</em><em>default='L2'</em><em>)</em>) – It is the name of the distance used to compute the difference between the mixture of the training
distribution and the testing distribution</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>(</em><em>default=1e-05</em><em>)</em>) – The precision of the solution when search is used to compute the prevalence</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>(</em><em>default=0</em><em>)</em>) – The verbosity level. The default value, zero, means silent mode</p></li>
<li><p><strong>same</strong> (<em>For some experiments both estimators could be the</em>) – </p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.friedman.FriedmanME.estimator_train">
<span class="sig-name descname"><span class="pre">estimator_train</span></span><a class="headerlink" href="#quantificationlib.multiclass.friedman.FriedmanME.estimator_train" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimator used to classify the examples of the training set</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>estimator</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.friedman.FriedmanME.estimator_test">
<span class="sig-name descname"><span class="pre">estimator_test</span></span><a class="headerlink" href="#quantificationlib.multiclass.friedman.FriedmanME.estimator_test" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimator used to classify the examples of the testing bag</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>estimator</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.friedman.FriedmanME.predictions_train_">
<span class="sig-name descname"><span class="pre">predictions_train_</span></span><a class="headerlink" href="#quantificationlib.multiclass.friedman.FriedmanME.predictions_train_" title="Permalink to this definition">¶</a></dt>
<dd><p>Predictions of the examples in the training set</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ndarray, shape (n_examples, n_classes) (probabilistic estimator)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.friedman.FriedmanME.predictions_test_">
<span class="sig-name descname"><span class="pre">predictions_test_</span></span><a class="headerlink" href="#quantificationlib.multiclass.friedman.FriedmanME.predictions_test_" title="Permalink to this definition">¶</a></dt>
<dd><p>Predictions of the examples in the testing bag</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ndarray, shape (n_examples, n_classes) (probabilistic estimator)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.friedman.FriedmanME.needs_predictions_train">
<span class="sig-name descname"><span class="pre">needs_predictions_train</span></span><a class="headerlink" href="#quantificationlib.multiclass.friedman.FriedmanME.needs_predictions_train" title="Permalink to this definition">¶</a></dt>
<dd><p>It is True because FriedmanME quantifiers need to estimate the training distribution</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>bool, True</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.friedman.FriedmanME.probabilistic_predictions">
<span class="sig-name descname"><span class="pre">probabilistic_predictions</span></span><a class="headerlink" href="#quantificationlib.multiclass.friedman.FriedmanME.probabilistic_predictions" title="Permalink to this definition">¶</a></dt>
<dd><p>This means that <a href="#id145"><span class="problematic" id="id146">predictions_train_</span></a>/<a href="#id147"><span class="problematic" id="id148">predictions_test_</span></a> contain probabilistic predictions</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>bool, True</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.friedman.FriedmanME.distance">
<span class="sig-name descname"><span class="pre">distance</span></span><a class="headerlink" href="#quantificationlib.multiclass.friedman.FriedmanME.distance" title="Permalink to this definition">¶</a></dt>
<dd><p>The name of the distance function used</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>A distance function (default=l2)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.friedman.FriedmanME.tol">
<span class="sig-name descname"><span class="pre">tol</span></span><a class="headerlink" href="#quantificationlib.multiclass.friedman.FriedmanME.tol" title="Permalink to this definition">¶</a></dt>
<dd><p>The precision of the solution when search is used to compute the solution</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.friedman.FriedmanME.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#quantificationlib.multiclass.friedman.FriedmanME.classes_" title="Permalink to this definition">¶</a></dt>
<dd><p>Class labels</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ndarray, shape (n_classes, )</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.friedman.FriedmanME.y_ext_">
<span class="sig-name descname"><span class="pre">y_ext_</span></span><a class="headerlink" href="#quantificationlib.multiclass.friedman.FriedmanME.y_ext_" title="Permalink to this definition">¶</a></dt>
<dd><p>Repmat of true labels of the training set. When CV_estimator is used with averaged_predictions=False,
<a href="#id149"><span class="problematic" id="id150">predictions_train_</span></a> will have a larger dimension (factor=n_repetitions * n_folds of the underlying CV)
than y. In other cases, <a href="#id151"><span class="problematic" id="id152">y_ext_</span></a> == y.
<a href="#id153"><span class="problematic" id="id154">y_ext_</span></a> i used in <cite>fit</cite> method whenever the true labels of the training set are needed, instead of y</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ndarray, shape(len(<a href="#id155"><span class="problematic" id="id156">predictions_train_</span></a>, 1)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.friedman.FriedmanME.train_prevs_">
<span class="sig-name descname"><span class="pre">train_prevs_</span></span><a class="headerlink" href="#quantificationlib.multiclass.friedman.FriedmanME.train_prevs_" title="Permalink to this definition">¶</a></dt>
<dd><p>Prevalence of each class in the training set</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ndarray, shape (n_classes, )</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.friedman.FriedmanME.train_distrib_">
<span class="sig-name descname"><span class="pre">train_distrib_</span></span><a class="headerlink" href="#quantificationlib.multiclass.friedman.FriedmanME.train_distrib_" title="Permalink to this definition">¶</a></dt>
<dd><p>Each column is the representation of the training examples of such class. The column contains the
percentage of examples of each class whose probability to belong to the row class is &gt;= than
the prevalence of the row class in the training set</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ndarray, shape (n_classes, n_classes)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.friedman.FriedmanME.test_distrib_">
<span class="sig-name descname"><span class="pre">test_distrib_</span></span><a class="headerlink" href="#quantificationlib.multiclass.friedman.FriedmanME.test_distrib_" title="Permalink to this definition">¶</a></dt>
<dd><p>Percentage of examples in the testing bag whose probability to belong each class is &gt;= than
the prevalence of that class in the training set</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ndarray, shape (<a href="#id157"><span class="problematic" id="id158">n_classes_</span></a>, 1)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">G_,</span> <span class="pre">C_,</span> <span class="pre">b_</span></span></dt>
<dd><p>These variables are precomputed in the <cite>fit</cite> method and are used for solving the optimization problem
using <cite>quadprog.solve_qp</cite>. See <cite>compute_l2_param_train</cite> function</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>variables of different kind for definining the optimization problem</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.friedman.FriedmanME.problem_">
<span class="sig-name descname"><span class="pre">problem_</span></span><a class="headerlink" href="#quantificationlib.multiclass.friedman.FriedmanME.problem_" title="Permalink to this definition">¶</a></dt>
<dd><p>This attribute is set to None in the fit() method. With such model, the first time a testing bag is
predicted this attribute will contain the corresponding cvxpy Object (if such library is used, i.e in the
case of ‘L1’ and ‘HD’). For the rest testing bags, this object is passed to allow a warm start. The
solving process is faster.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>a cvxpy Problem object</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.friedman.FriedmanME.mixtures_">
<span class="sig-name descname"><span class="pre">mixtures_</span></span><a class="headerlink" href="#quantificationlib.multiclass.friedman.FriedmanME.mixtures_" title="Permalink to this definition">¶</a></dt>
<dd><p>Contains the mixtures for all the prevalences in the range [0, 1] step=0.01. This speeds up the prediction
for a collection of testing bags</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ndarray, shape (101, n_quantiles)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.friedman.FriedmanME.verbose">
<span class="sig-name descname"><span class="pre">verbose</span></span><a class="headerlink" href="#quantificationlib.multiclass.friedman.FriedmanME.verbose" title="Permalink to this definition">¶</a></dt>
<dd><p>The verbosity level</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Notes</p>
<p>Notice that at least one between estimator_train/predictions_train and estimator_test/predictions_test
must be not None. If both are None a ValueError exception will be raised. If both are not None,
predictions_train/predictions_test are used</p>
<p class="rubric">References</p>
<p>Jerome H. Friedman. Class counts in future unlabeled samples. Presentation at MIT CSAIL Big Data Event, 2014.</p>
<dl class="py method">
<dt class="sig sig-object py" id="quantificationlib.multiclass.friedman.FriedmanME.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predictions_train</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/quantificationlib/multiclass/friedman.html#FriedmanME.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#quantificationlib.multiclass.friedman.FriedmanME.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>This method performs the following operations: 1) fits the estimators for the training set and the
testing set (if needed), and 2) computes <a href="#id159"><span class="problematic" id="id160">predictions_train_</span></a> (probabilities) if needed. Both operations are
performed by the <a href="#id3"><span class="problematic" id="id4">`</span></a>fit method of its superclass.
Then, the method computes the training distribution of the method ME suggested by Friedman. The distribution
of a class contains the percentage of the training examples of that class whose probability to belong
to each class is &gt;= than the prevalence of such class in the training set
Finally, the method computes all the parameters for solving the optimization problem needed by quadprog
that do not need the testing distribution</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>n_features</em><em>)</em>) – Data</p></li>
<li><p><strong>y</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>)</em>) – True classes</p></li>
<li><p><strong>predictions_train</strong> (<em>ndarray</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>n_classes</em><em>)</em>) – Predictions of the training set</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – When estimator_train and predictions_train are both None</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="quantificationlib.multiclass.friedman.FriedmanME.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predictions_test</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/quantificationlib/multiclass/friedman.html#FriedmanME.predict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#quantificationlib.multiclass.friedman.FriedmanME.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict the class distribution of a testing bag</p>
<p>First, <a href="#id161"><span class="problematic" id="id162">predictions_test_</span></a> are computed (if needed, when predictions_test parameter is None) by
<a href="#id5"><span class="problematic" id="id6">`</span></a>super().predict() method.</p>
<p>After that, the method computes the distribution of the FriedmanME method for the testing bag. That is,
the percentage of examples in the testing bag whose probability to belong each class is &gt;= than
the prevalence of that class in the training set</p>
<p>Finally, the prevalences are computed solving the resulting optimization problem</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>n_features</em><em>)</em>) – Testing bag</p></li>
<li><p><strong>predictions_test</strong> (<em>ndarray</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>n_classes</em><em>) </em><em>(</em><em>default=None</em><em>)</em>) – <p>They must be probabilities (the estimator used must have a <cite>predict_proba</cite> method)</p>
<p>If predictions_test is not None they are copied on <a href="#id163"><span class="problematic" id="id164">predictions_test_</span></a> and used.
If predictions_test is None, predictions for the testing examples are computed using the <cite>predict</cite>
method of estimator_test (it must be an actual estimator)</p>
</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – When estimator_test and predictions_test are both None</p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>prevalences</strong> – Contains the predicted prevalence for each class</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>ndarray, shape(n_classes, )</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-quantificationlib.multiclass.knn">
<span id="quantificationlib-multiclass-knn-module"></span><h2>quantificationlib.multiclass.knn module<a class="headerlink" href="#module-quantificationlib.multiclass.knn" title="Permalink to this heading">¶</a></h2>
<p>PWKQuantifier a quantifier based on K-Nearest Neighbor</p>
<dl class="py class">
<dt class="sig sig-object py" id="quantificationlib.multiclass.knn.PWKQuantifier">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">PWKQuantifier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_neighbors</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'minkowski'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/quantificationlib/multiclass/knn.html#PWKQuantifier"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#quantificationlib.multiclass.knn.PWKQuantifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="rsts_t4/quantificationlib.base.html#quantificationlib.base.WithoutClassifiers" title="quantificationlib.base.WithoutClassifiers"><code class="xref py py-class docutils literal notranslate"><span class="pre">WithoutClassifiers</span></code></a></p>
<p>Quantifier based on K-Nearest Neighbor proposed by (Barranquero et al., 2013)</p>
<p>It is a AC method in which the estimator is PWK, a weighted version of KNN in
which the weight depends on the proportion of each class in the training set. It is not derived from AC to
allow decomposition</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_neighbors</strong> (<em>int</em><em>, </em><em>(</em><em>default=10</em><em>)</em>) – Number of neighbors to use by default for <code class="xref py py-meth docutils literal notranslate"><span class="pre">kneighbors()</span></code> queries.</p></li>
<li><p><strong>p</strong> (<em>int</em><em>, </em><em>default=2</em>) – Power parameter for the Minkowski metric. When p = 1, this is
equivalent to using manhattan_distance (l1), and euclidean_distance
(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.</p></li>
<li><p><strong>metric</strong> (<em>str</em><em> or </em><em>callable</em><em>, </em><em>default='minkowski'</em>) – The distance metric to use for the tree.  The default metric is
minkowski, and with p=2 is equivalent to the standard Euclidean
metric. For a list of available metrics, see the documentation of
<code class="xref py py-class docutils literal notranslate"><span class="pre">DistanceMetric</span></code>.
If metric is “precomputed”, X is assumed to be a distance matrix and
must be square during fit. X may be a <span class="xref std std-term">sparse graph</span>,
in which case only “nonzero” elements may be considered neighbors.</p></li>
<li><p><strong>metric_params</strong> (<em>dict</em><em>, </em><em>default=None</em>) – Additional keyword arguments for the metric function.</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>(</em><em>default=0</em><em>)</em>) – The verbosity level. The default value, zero, means silent mode</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.knn.PWKQuantifier.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#quantificationlib.multiclass.knn.PWKQuantifier.classes_" title="Permalink to this definition">¶</a></dt>
<dd><p>Class labels</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ndarray, shape (n_classes, )</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.knn.PWKQuantifier.cm_">
<span class="sig-name descname"><span class="pre">cm_</span></span><a class="headerlink" href="#quantificationlib.multiclass.knn.PWKQuantifier.cm_" title="Permalink to this definition">¶</a></dt>
<dd><p>Confusion matrix. The true classes are in the rows and the predicted classes in the columns. So, for
the binary case, the count of true negatives is cm_[0,0], false negatives is cm_[1,0],
true positives is cm_[1,1] and false positives is cm_[0,1] .</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ndarray, shape (n_classes, n_classes)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.knn.PWKQuantifier.problem_">
<span class="sig-name descname"><span class="pre">problem_</span></span><a class="headerlink" href="#quantificationlib.multiclass.knn.PWKQuantifier.problem_" title="Permalink to this definition">¶</a></dt>
<dd><p>This attribute is set to None in the fit() method. With such model, the first time a testing bag is
predicted this attribute will contain the corresponding cvxpy Object (if such library is used, i.e in the
case of ‘L1’ and ‘HD’). For the rest testing bags, this object is passed to allow a warm start. The
solving process is faster.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>a cvxpy Problem object</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="quantificationlib.multiclass.knn.PWKQuantifier.verbose">
<span class="sig-name descname"><span class="pre">verbose</span></span><a class="headerlink" href="#quantificationlib.multiclass.knn.PWKQuantifier.verbose" title="Permalink to this definition">¶</a></dt>
<dd><p>The verbosity level</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">References</p>
<p>Jose Barranquero, Pablo González, Jorge Díez, Juna José del Coz: On the study of nearest neighbor algorithms
for prevalence estimation in binary problems. Pattern Recognition, 46(2), 472-482. 2013</p>
<dl class="py method">
<dt class="sig sig-object py" id="quantificationlib.multiclass.knn.PWKQuantifier.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/quantificationlib/multiclass/knn.html#PWKQuantifier.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#quantificationlib.multiclass.knn.PWKQuantifier.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>This method performs the following operations: 1) fits the estimators for the training set and the
testing set (if needed), and 2) computes <a href="#id165"><span class="problematic" id="id166">predictions_train_</span></a> (crisp values) if needed. Both operations are
performed by the <cite>fit</cite> method of its superclass.
Finally the method computes the confusion matrix of the training set using <a href="#id167"><span class="problematic" id="id168">predictions_train_</span></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>n_features</em><em>)</em>) – Data</p></li>
<li><p><strong>y</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>)</em>) – True classes</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="quantificationlib.multiclass.knn.PWKQuantifier.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/quantificationlib/multiclass/knn.html#PWKQuantifier.predict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#quantificationlib.multiclass.knn.PWKQuantifier.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict the class distribution of a testing bag</p>
<p>The prevalences are computed solving a system of linear scalar equations:</p>
<blockquote>
<div><p><a href="#id169"><span class="problematic" id="id170">cm_</span></a>.T * prevalences = CC(X)</p>
</div></blockquote>
<p>For binary problems the system is directly solved using the original AC algorithm proposed by Forman</p>
<blockquote>
<div><p>p = (p_0 - fpr ) / ( tpr - fpr)</p>
</div></blockquote>
<p>For multiclass problems, the system may not have a solution. Thus, instead we propose to solve an
optimization problem of this kind:</p>
<blockquote>
<div><p>Min   distance ( <a href="#id171"><span class="problematic" id="id172">cm_</span></a>.T * prevalences, CC(X) )
s.t.  sum(prevalences) = 1</p>
<blockquote>
<div><p>prevalecences_i &gt;= 0</p>
</div></blockquote>
</div></blockquote>
<p>in which distance is ‘L1’</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>X</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>n_features</em><em>)</em>) – Testing bag</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>prevalences</strong> – Contains the predicted prevalence for each class</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ndarray, shape(n_classes, )</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-quantificationlib.multiclass.regression">
<span id="quantificationlib-multiclass-regression-module"></span><h2>quantificationlib.multiclass.regression module<a class="headerlink" href="#module-quantificationlib.multiclass.regression" title="Permalink to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="quantificationlib.multiclass.regression.REG">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">REG</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">bag_generator=&lt;quantificationlib.bag_generator.PriorShift_BagGenerator</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_bins=8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bin_strategy='equal_width'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">regression_estimator=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose=0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/quantificationlib/multiclass/regression.html#REG"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#quantificationlib.multiclass.regression.REG" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>REG base class for REGX y REGy</p>
<blockquote>
<div><p>The idea of these quantifiers is to learn a regression model able to predict the prevalences. To learn said
regression model, this kind of objects generates a training set of bag of examples using a selected kind of
shift (prior probability shift, covariate shift or a mix of both). The training set contains a collection of
pairs (PDF distribution, prevalences) in which each pair is obtained from a bag of examples. The PDF tries
to capture the distribution of the bag.</p>
<dl>
<dt>bag_generator<span class="classifier">BagGenerator object (default=PriorShift_BagGenerator())</span></dt><dd><p>Object to generate the bags with a selected shift</p>
</dd>
<dt>n_bins<span class="classifier">int (default=8)</span></dt><dd><p>Number of bins to compute the PDF of each distribution</p>
</dd>
<dt>bin_strategy<span class="classifier">str (default=’normal’)</span></dt><dd><dl>
<dt>Method to compute the boundaries of the bins:</dt><dd><p>‘equal_width’: bins of equal length (it could be affected by outliers)
‘equal_count’: bins of equal counts (considering the examples of all classes)
‘binormal’: (Only for binary quantification) It is inspired on the method devised by</p>
<blockquote>
<div><blockquote>
<div><p>(Tasche, 2019, Eq (A16b)). the cut points, $-infty &lt; c_1 &lt; ldots &lt; c_{b-1} &lt; infty$,
are computed as follows based on the assumption that the features follow a normal
distribution:</p>
</div></blockquote>
<p>$c_i =</p>
</div></blockquote>
</dd>
</dl>
</dd>
</dl>
</div></blockquote>
<p>rac{sigma^+ + sigma^{-}}{2} Phi^{-1}igg(
rac{i}{b}igg)  +
rac{mu^+ + mu^{-}}{2} ,  quad i=1,ldots,b-1$</p>
<blockquote>
<div><blockquote>
<div><p>where $Phi^{-1}$ is the quantile function of the standard normal distribution, and $mu$
and $sigma$ of the normal distribution are estimated as the average of those values for
the training examples of each class.</p>
</div></blockquote>
<dl>
<dt>‘normal’:  The idea is that each feacture follows a normal distribution. $mu$ and $sigma$ are</dt><dd><p>estimated as the weighted mean and std from the training distribution. The cut points
$-infty &lt; c_1 &lt; ldots &lt; c_{b-1} &lt; infty$ are computed as follows:</p>
<p>$c_i = sigma^ Phi^{-1}igg(</p>
</dd>
</dl>
</div></blockquote>
<p>rac{i}{b}igg)  + mu ,  quad i=1,ldots,b-1$</p>
<blockquote>
<div><dl class="simple">
<dt>regression_estimator: estimator object (default=None)</dt><dd><p>A regression estimator object. If the value is None the regression estimator used is a Generalized Linear
Model (GLM) from statsmodels package with logit link and Binomial family as parameters (see Baum 2008).
It is used to learn a regression model able to predict the prevalence for each class, so the method will
fit as many regression estimators as classes in multiclass problems and just one for binary problems.</p>
</dd>
<dt>verbose<span class="classifier">int, optional, (default=0)</span></dt><dd><p>The verbosity level. The default value, zero, means silent mode</p>
</dd>
</dl>
<dl class="simple">
<dt>bag_generator<span class="classifier">BagGenerator object</span></dt><dd><p>Object to generate the bags with a selected shift</p>
</dd>
<dt>n_bins<span class="classifier">int</span></dt><dd><p>Number of bins to compute the PDF of each distribution</p>
</dd>
<dt>bin_strategy<span class="classifier">str</span></dt><dd><p>Method to compute the boundaries of the bins</p>
</dd>
<dt>regression_estimator: estimator object, None</dt><dd><p>A regression estimator object</p>
</dd>
<dt>verbose<span class="classifier">int</span></dt><dd><p>The verbosity level</p>
</dd>
<dt><a href="#id173"><span class="problematic" id="id174">dataX_</span></a><span class="classifier">array-like, shape(n_bags, n_features)</span></dt><dd><p>X data for training REGX/REGy’s regressor model. Each row corresponds to the collection of histograms (one
per input feature) of the corresponding bag</p>
</dd>
<dt><a href="#id175"><span class="problematic" id="id176">dataY_</span></a><span class="classifier">array-like, shape(n_bags, n_classes)</span></dt><dd><p>Y data for training REGX/REGy’s regressor model. Each value corresponds to the prevalences of the
corresponding bag</p>
</dd>
<dt><a href="#id177"><span class="problematic" id="id178">bincuts_</span></a><span class="classifier">ndarray, shape (n_features, n_bins + 1)</span></dt><dd><p>Bin cuts for each feature</p>
</dd>
<dt><a href="#id179"><span class="problematic" id="id180">estimators_</span></a><span class="classifier">array of estimators, shape (n_classes, ) multiclass (1, ) binary quantification</span></dt><dd><p>It stores the estimators. For multiclass problems, the method learns an individual estimator
for each class</p>
</dd>
<dt><a href="#id181"><span class="problematic" id="id182">models_</span></a><span class="classifier">array of models, i.e., fitted estimators, shape (n_classes, )</span></dt><dd><p>This is the fitted regressor model for each class. It is needed when regression_estimator is None and
a GML models are used (these objects do not store the fitted model).</p>
</dd>
<dt><a href="#id183"><span class="problematic" id="id184">n_classes_</span></a><span class="classifier">int</span></dt><dd><p>The number of classes</p>
</dd>
</dl>
<p>Christopher F. Baum: Stata tip 63: Modeling proportions. The Stata Journal 8.2 (2008): 299-303</p>
</div></blockquote>
<dl class="py method">
<dt class="sig sig-object py" id="quantificationlib.multiclass.regression.REG.create_training_set_of_distributions">
<span class="sig-name descname"><span class="pre">create_training_set_of_distributions</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">att_range</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/quantificationlib/multiclass/regression.html#REG.create_training_set_of_distributions"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#quantificationlib.multiclass.regression.REG.create_training_set_of_distributions" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a training set for REG objects. Each example corresponds to a histogram of a bag
of examples generated from (X, y). The size of the complete histogram is n_features * n_bins, because
it is formed by concatenating the histogram for each input feature. This method computes the values
for <a href="#id185"><span class="problematic" id="id186">dataX_</span></a>, <a href="#id187"><span class="problematic" id="id188">dataY_</span></a> and <a href="#id189"><span class="problematic" id="id190">bincuts_</span></a> attributes</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>n_features</em><em>)</em>) – Data</p></li>
<li><p><strong>y</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>)</em>) – True classes</p></li>
<li><p><strong>att_range</strong> (<em>array-like</em><em>, </em><em>(</em><em>2</em><em>,</em><em>1</em><em>)</em>) – Min and Max possible values of the input feature x. These values might not coincide with the actual
Min and Max values of vector x. For instance, in the case of x represents a set of probabilistic
predictions, these values will be 0 and 1. These values may be needed by compute_bincuts function</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="quantificationlib.multiclass.regression.REG.fit_regressor">
<span class="sig-name descname"><span class="pre">fit_regressor</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/quantificationlib/multiclass/regression.html#REG.fit_regressor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#quantificationlib.multiclass.regression.REG.fit_regressor" title="Permalink to this definition">¶</a></dt>
<dd><p>This method trains the regressor model using <a href="#id191"><span class="problematic" id="id192">dataX_</span></a> and <a href="#id193"><span class="problematic" id="id194">dataY_</span></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="quantificationlib.multiclass.regression.REG.predict_bag">
<span class="sig-name descname"><span class="pre">predict_bag</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">bagX</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/quantificationlib/multiclass/regression.html#REG.predict_bag"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#quantificationlib.multiclass.regression.REG.predict_bag" title="Permalink to this definition">¶</a></dt>
<dd><p>This method makes a prediction for a testing bag represented by its PDF, bagX parameter</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>bagX</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_bins * n_classes</em><em>, </em><em>) </em><em>for REGy and</em><em> (</em><em>n_bins * n_features</em><em>, </em><em>) </em><em>for REGX</em>) – Testing bag’s PDF</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>prevalences</strong> – Contains the predicted prevalence for each class</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ndarray, shape(n_classes, )</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="quantificationlib.multiclass.regression.REGX">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">REGX</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">bag_generator=&lt;quantificationlib.bag_generator.PriorShift_BagGenerator</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_bins=8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bin_strategy='normal'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">regression_estimator=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose=False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/quantificationlib/multiclass/regression.html#REGX"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#quantificationlib.multiclass.regression.REGX" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="rsts_t4/quantificationlib.base.html#quantificationlib.base.WithoutClassifiers" title="quantificationlib.base.WithoutClassifiers"><code class="xref py py-class docutils literal notranslate"><span class="pre">WithoutClassifiers</span></code></a>, <a class="reference internal" href="rsts_t4/quantificationlib.multiclass.regression.html#quantificationlib.multiclass.regression.REG" title="quantificationlib.multiclass.regression.REG"><code class="xref py py-class docutils literal notranslate"><span class="pre">REG</span></code></a></p>
<blockquote>
<div><p>The idea is to learn a regression model able to predict the prevalences given a PDF distribution. In this case,
the distributions are represented using PDFs of the input features (X). To learn such regression model, this
object generates a training set of bags of examples using a selected kind of shift (prior probability shift,
covariate shift or a mix of both)</p>
<dl>
<dt>bag_generator<span class="classifier">BagGenerator object (default=PriorShift_BagGenerator())</span></dt><dd><p>Object to generate the bags with a selected shift</p>
</dd>
<dt>n_bins<span class="classifier">int (default=8)</span></dt><dd><p>Number of bins to compute the PDF of each distribution</p>
</dd>
<dt>bin_strategy<span class="classifier">str (default=’normal’)</span></dt><dd><dl>
<dt>Method to compute the boundaries of the bins:</dt><dd><p>‘equal_width’: bins of equal length (it could be affected by outliers)
‘equal_count’: bins of equal counts (considering the examples of all classes)
‘binormal’: (Only for binary quantification) It is inspired on the method devised by</p>
<blockquote>
<div><blockquote>
<div><p>(Tasche, 2019, Eq (A16b)). the cut points, $-infty &lt; c_1 &lt; ldots &lt; c_{b-1} &lt; infty$,
are computed as follows based on the assumption that the features follow a normal
distribution:</p>
</div></blockquote>
<p>$c_i =</p>
</div></blockquote>
</dd>
</dl>
</dd>
</dl>
</div></blockquote>
<p>rac{sigma^+ + sigma^{-}}{2} Phi^{-1}igg(
rac{i}{b}igg)  +
rac{mu^+ + mu^{-}}{2} ,  quad i=1,ldots,b-1$</p>
<blockquote>
<div><blockquote>
<div><p>where $Phi^{-1}$ is the quantile function of the standard normal distribution, and $mu$
and $sigma$ of the normal distribution are estimated as the average of those values for
the training examples of each class.</p>
</div></blockquote>
<dl>
<dt>‘normal’:  The idea is that each feacture follows a normal distribution. $mu$ and $sigma$ are</dt><dd><p>estimated as the weighted mean and std from the training distribution. The cut points
$-infty &lt; c_1 &lt; ldots &lt; c_{b-1} &lt; infty$ are computed as follows:</p>
<p>$c_i = sigma^ Phi^{-1}igg(</p>
</dd>
</dl>
</div></blockquote>
<p>rac{i}{b}igg)  + mu ,  quad i=1,ldots,b-1$</p>
<blockquote>
<div><dl class="simple">
<dt>regression_estimator: estimator object (default=None)</dt><dd><p>A regression estimator object. If the value is None the regression estimator used is a Generalized Linear
Model (GLM) from statsmodels package with logit link and Binomial family as parameters (see Baum 2008).
It is used to learn a regression model able to predict the prevalence for each class, so the method will
fit as many regression estimators as classes in multiclass problems and just one for binary problems.</p>
</dd>
<dt>verbose<span class="classifier">int, optional, (default=0)</span></dt><dd><p>The verbosity level. The default value, zero, means silent mode</p>
</dd>
</dl>
<dl class="simple">
<dt>bag_generator<span class="classifier">BagGenerator object</span></dt><dd><p>Object to generate the bags with a selected shift</p>
</dd>
<dt>n_bins<span class="classifier">int</span></dt><dd><p>Number of bins to compute the PDF of each distribution</p>
</dd>
<dt>bin_strategy<span class="classifier">str</span></dt><dd><p>Method to compute the boundaries of the bins</p>
</dd>
<dt>regression_estimator: estimator object, None</dt><dd><p>A regression estimator object</p>
</dd>
<dt>verbose<span class="classifier">int</span></dt><dd><p>The verbosity level</p>
</dd>
<dt><a href="#id195"><span class="problematic" id="id196">dataX_</span></a><span class="classifier">array-like, shape(n_bags, n_features)</span></dt><dd><p>X data for training REGX’s regressor model. Each row corresponds to the collection of histograms (one
per input feature) of the corresponding bag</p>
</dd>
<dt><a href="#id197"><span class="problematic" id="id198">dataY_</span></a><span class="classifier">array-like, shape(n_bags, n_classes)</span></dt><dd><p>Y data for training REGX’s regressor model. Each value corresponds to the prevalences of the
corresponding bag</p>
</dd>
<dt><a href="#id199"><span class="problematic" id="id200">bincuts_</span></a><span class="classifier">ndarray, shape (n_features, n_bins + 1)</span></dt><dd><p>Bin cuts for each feature</p>
</dd>
<dt><a href="#id201"><span class="problematic" id="id202">estimators_</span></a><span class="classifier">array of estimators, shape (n_classes, ) multiclass (1, ) binary quantification</span></dt><dd><p>It stores the estimators. For multiclass problems, the method learns an individual estimator
for each class</p>
</dd>
<dt><a href="#id203"><span class="problematic" id="id204">models_</span></a><span class="classifier">array of models, i.e., fitted estimators, shape (n_classes, )</span></dt><dd><p>This is the fitted regressor model for each class. It is needed when regression_estimator is None and
a GML models are used (this objects do not store the fitted model).</p>
</dd>
<dt><a href="#id205"><span class="problematic" id="id206">n_classes_</span></a><span class="classifier">int</span></dt><dd><p>The number of classes</p>
</dd>
</dl>
<p>Christopher F. Baum: Stata tip 63: Modeling proportions. The Stata Journal 8.2 (2008): 299-303</p>
</div></blockquote>
<dl class="py method">
<dt class="sig sig-object py" id="quantificationlib.multiclass.regression.REGX.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/quantificationlib/multiclass/regression.html#REGX.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#quantificationlib.multiclass.regression.REGX.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>This method just has two steps: 1) it computes a training dataset formed by a collection of bags of
examples (using create_training_set_of_distributions) and 2) it trains a regression model using said
training set just calling fit_regressor, a inherited method from REG base class</p>
<blockquote>
<div><dl class="simple">
<dt>X<span class="classifier">array-like, shape (n_examples, n_features)</span></dt><dd><p>Data</p>
</dd>
<dt>y<span class="classifier">array-like, shape (n_examples, )</span></dt><dd><p>True classes</p>
</dd>
</dl>
</div></blockquote>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="quantificationlib.multiclass.regression.REGX.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/quantificationlib/multiclass/regression.html#REGX.predict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#quantificationlib.multiclass.regression.REGX.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>This method computes the histogram for the testing set X, using the bincuts for each input feature
computed by fit method and then it makes a prediction applying the regression model using the
inherited method predict_bag</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>X</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>n_features</em><em>)</em>) – Testing bag</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>prevalences</strong> – Contains the predicted prevalence for each class</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ndarray, shape(n_classes, )</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="quantificationlib.multiclass.regression.REGy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">REGy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">estimator_train=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator_test=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bag_generator=&lt;quantificationlib.bag_generator.PriorShift_BagGenerator</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_bins=8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bin_strategy='equal_width'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">regression_estimator=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose=False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/quantificationlib/multiclass/regression.html#REGy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#quantificationlib.multiclass.regression.REGy" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="rsts_t4/quantificationlib.base.html#quantificationlib.base.UsingClassifiers" title="quantificationlib.base.UsingClassifiers"><code class="xref py py-class docutils literal notranslate"><span class="pre">UsingClassifiers</span></code></a>, <a class="reference internal" href="rsts_t4/quantificationlib.multiclass.regression.html#quantificationlib.multiclass.regression.REG" title="quantificationlib.multiclass.regression.REG"><code class="xref py py-class docutils literal notranslate"><span class="pre">REG</span></code></a></p>
<blockquote>
<div><p>The idea is to learn a regression model able to predict the prevalences given a PDF distribution. In this case,
the distributions are represented using PDFs of the predictions (y) from a classifer. To learn such regression
model, this object first trains a classifier using all data and then generates a training set of bags of
examples (in this case the predictions of each example) using a selected kind of shift (prior probability
shift, covariate shift or a mix of both)</p>
<dl>
<dt>estimator_train<span class="classifier">estimator object (default=None)</span></dt><dd><p>An estimator object implementing <cite>fit</cite> and <cite>predict_proba</cite>. It is used to train a classifier using the
examples of the training set. This classifier is used to obtain the predictions for the training
examples and to compute the PDF of each class individually using such predictions</p>
</dd>
<dt>estimator_test<span class="classifier">estimator object (default=None)</span></dt><dd><p>An estimator object implementing <cite>fit</cite> and <cite>predict_proba</cite>. It is used to classify the examples of the
testing set and to compute the distribution of the whole testing set</p>
</dd>
<dt>bag_generator<span class="classifier">BagGenerator object (default=PriorShift_BagGenerator())</span></dt><dd><p>Object to generate the bags with a selected shift</p>
</dd>
<dt>n_bins<span class="classifier">int (default=8)</span></dt><dd><p>Number of bins to compute the PDF of each distribution</p>
</dd>
<dt>bin_strategy<span class="classifier">str (default=’normal’)</span></dt><dd><dl>
<dt>Method to compute the boundaries of the bins:</dt><dd><p>‘equal_width’: bins of equal length (it could be affected by outliers)
‘equal_count’: bins of equal counts (considering the examples of all classes)
‘binormal’: (Only for binary quantification) It is inspired on the method devised by</p>
<blockquote>
<div><blockquote>
<div><p>(Tasche, 2019, Eq (A16b)). the cut points, $-infty &lt; c_1 &lt; ldots &lt; c_{b-1} &lt; infty$,
are computed as follows based on the assumption that the features follow a normal
distribution:</p>
</div></blockquote>
<p>$c_i =</p>
</div></blockquote>
</dd>
</dl>
</dd>
</dl>
</div></blockquote>
<p>rac{sigma^+ + sigma^{-}}{2} Phi^{-1}igg(
rac{i}{b}igg)  +
rac{mu^+ + mu^{-}}{2} ,  quad i=1,ldots,b-1$</p>
<blockquote>
<div><blockquote>
<div><p>where $Phi^{-1}$ is the quantile function of the standard normal distribution, and $mu$
and $sigma$ of the normal distribution are estimated as the average of those values for
the training examples of each class.</p>
</div></blockquote>
<dl>
<dt>‘normal’:  The idea is that each feacture follows a normal distribution. $mu$ and $sigma$ are</dt><dd><p>estimated as the weighted mean and std from the training distribution. The cut points
$-infty &lt; c_1 &lt; ldots &lt; c_{b-1} &lt; infty$ are computed as follows:</p>
<p>$c_i = sigma^ Phi^{-1}igg(</p>
</dd>
</dl>
</div></blockquote>
<p>rac{i}{b}igg)  + mu ,  quad i=1,ldots,b-1$</p>
<blockquote>
<div><dl class="simple">
<dt>regression_estimator: estimator object (default=None)</dt><dd><p>A regression estimator object. If it is None the regression estimator used is a Generalized Linear
Model (GLM) from statsmodels package with logit link and Binomial family as parameters.
It is used to learn a regression model able to predict the prevalence for each class, so the method will
fit as many regression estimators as classes in multiclass problem and just one for binary problems.</p>
</dd>
<dt>verbose<span class="classifier">int, optional, (default=0)</span></dt><dd><p>The verbosity level. The default value, zero, means silent mode</p>
</dd>
</dl>
<dl class="simple">
<dt>estimator_train<span class="classifier">estimator</span></dt><dd><p>Estimator used to classify the examples of the training set</p>
</dd>
<dt>estimator_test<span class="classifier">estimator</span></dt><dd><p>Estimator used to classify the examples of the testing bag</p>
</dd>
<dt>bag_generator<span class="classifier">BagGenerator object</span></dt><dd><p>Object to generate the bags with a selected shift</p>
</dd>
<dt>needs_predictions_train<span class="classifier">bool, True</span></dt><dd><p>It is True because PDFy quantifiers need to estimate the training distribution</p>
</dd>
<dt>probabilistic_predictions<span class="classifier">bool, True</span></dt><dd><p>This means that <a href="#id207"><span class="problematic" id="id208">predictions_train_</span></a>/<a href="#id209"><span class="problematic" id="id210">predictions_test_</span></a> contain probabilistic predictions</p>
</dd>
<dt>n_bins<span class="classifier">int</span></dt><dd><p>Number of bins to compute the PDF of each distribution</p>
</dd>
<dt>bin_strategy<span class="classifier">str</span></dt><dd><p>Method to compute the boundaries of the bins</p>
</dd>
<dt>regression_estimator: estimator object, None</dt><dd><p>A regression estimator object</p>
</dd>
<dt>verbose<span class="classifier">int</span></dt><dd><p>The verbosity level</p>
</dd>
<dt><a href="#id211"><span class="problematic" id="id212">predictions_train_</span></a><span class="classifier">ndarray, shape (n_examples, n_classes) (probabilities)</span></dt><dd><p>Predictions of the examples in the training set</p>
</dd>
<dt><a href="#id213"><span class="problematic" id="id214">predictions_test_</span></a><span class="classifier">ndarray, shape (n_examples, n_classes) (probabilities)</span></dt><dd><p>Predictions of the examples in the testing bag</p>
</dd>
<dt><a href="#id215"><span class="problematic" id="id216">classes_</span></a><span class="classifier">ndarray, shape (n_classes, )</span></dt><dd><p>Class labels</p>
</dd>
<dt><a href="#id217"><span class="problematic" id="id218">dataX_</span></a><span class="classifier">array-like, shape(n_bags, n_features)</span></dt><dd><p>X data for training REGy’s regressor model. Each row corresponds to the predictions histogram for the
examples of the corresponding bag</p>
</dd>
<dt><a href="#id219"><span class="problematic" id="id220">dataY_</span></a><span class="classifier">array-like, shape(n_bags, n_classes)</span></dt><dd><p>Y data for training REGy’s regressor model. Each value corresponds to the prevalences of the
corresponding bag</p>
</dd>
<dt><a href="#id221"><span class="problematic" id="id222">bincuts_</span></a><span class="classifier">ndarray, shape (n_features, n_bins + 1)</span></dt><dd><p>Bin cuts for each feature</p>
</dd>
<dt><a href="#id223"><span class="problematic" id="id224">estimators_</span></a><span class="classifier">array of estimators, shape (n_classes, ) multiclass (1, ) binary quantification</span></dt><dd><p>It stores the estimators. For multiclass problems, the method learns an individual estimator
for each class</p>
</dd>
<dt><a href="#id225"><span class="problematic" id="id226">models_</span></a><span class="classifier">array of models, i.e., fitted estimators, shape (n_classes, )</span></dt><dd><p>This is the fitted regressor model for each class. It is needed when regression_estimator is None and
a GML models are used (this objects do not store the fitted model).</p>
</dd>
<dt><a href="#id227"><span class="problematic" id="id228">n_classes_</span></a><span class="classifier">int</span></dt><dd><p>The number of classes</p>
</dd>
</dl>
<p>Christopher F. Baum: Stata tip 63: Modeling proportions. The Stata Journal 8.2 (2008): 299-303</p>
</div></blockquote>
<dl class="py method">
<dt class="sig sig-object py" id="quantificationlib.multiclass.regression.REGy.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predictions_train</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/quantificationlib/multiclass/regression.html#REGy.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#quantificationlib.multiclass.regression.REGy.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>This method just has two steps: 1) it computes a training dataset formed by a collection of bags of
examples (using create_training_set_of_distributions) and 2) it trains a regression model using said
training set just calling fit_regressor, a inherited method from REG base class</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>n_features</em><em>)</em>) – Data</p></li>
<li><p><strong>y</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>)</em>) – True classes</p></li>
<li><p><strong>predictions_train</strong> (<em>ndarray</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>n_classes</em><em>)</em>) – Predictions of the examples in the training set</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – When estimator_train and predictions_train are both None</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="quantificationlib.multiclass.regression.REGy.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predictions_test</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/quantificationlib/multiclass/regression.html#REGy.predict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#quantificationlib.multiclass.regression.REGy.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>This method first computes the histogram for the testing set X, using the bincuts computed by the fit
method and the predictions for the testing bag (X, y). These predictions may be explicited given in the
predictions_test parameter. Then it makes a prediction applying the regression model using the
inherited method predict_bag</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>n_features</em><em>)</em>) – Testing bag</p></li>
<li><p><strong>predictions_test</strong> (<em>ndarray</em><em>, </em><em>shape</em><em> (</em><em>n_examples</em><em>, </em><em>n_classes</em><em>) </em><em>(</em><em>default=None</em><em>)</em>) – <p>They must be probabilities (the estimator used must have a <cite>predict_proba</cite> method)</p>
<p>If predictions_test is not None they are copied on <a href="#id229"><span class="problematic" id="id230">predictions_test_</span></a> and used.
If predictions_test is None, predictions for the testing examples are computed using the <cite>predict</cite>
method of estimator_test (it must be an actual estimator)</p>
</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – When estimator_test and predictions_test are both None</p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>prevalences</strong> – Contains the predicted prevalence for each class</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>ndarray, shape(n_classes, )</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-quantificationlib.multiclass">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-quantificationlib.multiclass" title="Permalink to this heading">¶</a></h2>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">





<h3><a href="index.html">Table of Contents</a></h3>
<ul>
<li><a class="reference internal" href="#">quantificationlib.multiclass package</a><ul>
<li><a class="reference internal" href="#submodules">Submodules</a></li>
<li><a class="reference internal" href="#module-quantificationlib.multiclass.df">quantificationlib.multiclass.df module</a><ul>
<li><a class="reference internal" href="#quantificationlib.multiclass.df.DFX"><code class="docutils literal notranslate"><span class="pre">DFX</span></code></a><ul>
<li><a class="reference internal" href="#quantificationlib.multiclass.df.DFX.fit"><code class="docutils literal notranslate"><span class="pre">fit()</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.df.DFX.predict"><code class="docutils literal notranslate"><span class="pre">predict()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#quantificationlib.multiclass.df.DFy"><code class="docutils literal notranslate"><span class="pre">DFy</span></code></a><ul>
<li><a class="reference internal" href="#quantificationlib.multiclass.df.DFy.fit"><code class="docutils literal notranslate"><span class="pre">fit()</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.df.DFy.predict"><code class="docutils literal notranslate"><span class="pre">predict()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#quantificationlib.multiclass.df.HDX"><code class="docutils literal notranslate"><span class="pre">HDX</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.df.HDy"><code class="docutils literal notranslate"><span class="pre">HDy</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.df.MMy"><code class="docutils literal notranslate"><span class="pre">MMy</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.df.compute_bincuts"><code class="docutils literal notranslate"><span class="pre">compute_bincuts()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#module-quantificationlib.multiclass.em">quantificationlib.multiclass.em module</a><ul>
<li><a class="reference internal" href="#quantificationlib.multiclass.em.EM"><code class="docutils literal notranslate"><span class="pre">EM</span></code></a><ul>
<li><a class="reference internal" href="#quantificationlib.multiclass.em.EM.estimator_train"><code class="docutils literal notranslate"><span class="pre">estimator_train</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.em.EM.estimator_test"><code class="docutils literal notranslate"><span class="pre">estimator_test</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.em.EM.predictions_train_"><code class="docutils literal notranslate"><span class="pre">predictions_train_</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.em.EM.predictions_test_"><code class="docutils literal notranslate"><span class="pre">predictions_test_</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.em.EM.needs_predictions_train"><code class="docutils literal notranslate"><span class="pre">needs_predictions_train</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.em.EM.probabilistic_predictions"><code class="docutils literal notranslate"><span class="pre">probabilistic_predictions</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.em.EM.classes_"><code class="docutils literal notranslate"><span class="pre">classes_</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.em.EM.y_ext_"><code class="docutils literal notranslate"><span class="pre">y_ext_</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.em.EM.verbose"><code class="docutils literal notranslate"><span class="pre">verbose</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.em.EM.epsilon_"><code class="docutils literal notranslate"><span class="pre">epsilon_</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.em.EM.max_iter_"><code class="docutils literal notranslate"><span class="pre">max_iter_</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.em.EM.prevalences_train_"><code class="docutils literal notranslate"><span class="pre">prevalences_train_</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.em.EM.fit"><code class="docutils literal notranslate"><span class="pre">fit()</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.em.EM.predict"><code class="docutils literal notranslate"><span class="pre">predict()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-quantificationlib.multiclass.energy">quantificationlib.multiclass.energy module</a><ul>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.CvMy"><code class="docutils literal notranslate"><span class="pre">CvMy</span></code></a><ul>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.CvMy.estimator_train"><code class="docutils literal notranslate"><span class="pre">estimator_train</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.CvMy.estimator_test"><code class="docutils literal notranslate"><span class="pre">estimator_test</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.CvMy.predictions_train_"><code class="docutils literal notranslate"><span class="pre">predictions_train_</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.CvMy.predictions_test_"><code class="docutils literal notranslate"><span class="pre">predictions_test_</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.CvMy.needs_predictions_train"><code class="docutils literal notranslate"><span class="pre">needs_predictions_train</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.CvMy.probabilistic_predictions"><code class="docutils literal notranslate"><span class="pre">probabilistic_predictions</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.CvMy.classes_"><code class="docutils literal notranslate"><span class="pre">classes_</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.CvMy.y_ext_"><code class="docutils literal notranslate"><span class="pre">y_ext_</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.CvMy.distance"><code class="docutils literal notranslate"><span class="pre">distance</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.CvMy.train_n_cls_i_"><code class="docutils literal notranslate"><span class="pre">train_n_cls_i_</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.CvMy.train_distrib_"><code class="docutils literal notranslate"><span class="pre">train_distrib_</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.CvMy.test_distrib_"><code class="docutils literal notranslate"><span class="pre">test_distrib_</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.CvMy.K_"><code class="docutils literal notranslate"><span class="pre">K_</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.CvMy.a_"><code class="docutils literal notranslate"><span class="pre">a_</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.CvMy.verbose"><code class="docutils literal notranslate"><span class="pre">verbose</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.CvMy.fit"><code class="docutils literal notranslate"><span class="pre">fit()</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.CvMy.predict"><code class="docutils literal notranslate"><span class="pre">predict()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.EDX"><code class="docutils literal notranslate"><span class="pre">EDX</span></code></a><ul>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.EDX.distance_"><code class="docutils literal notranslate"><span class="pre">distance_</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.EDX.classes_"><code class="docutils literal notranslate"><span class="pre">classes_</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.EDX.train_n_cls_i_"><code class="docutils literal notranslate"><span class="pre">train_n_cls_i_</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.EDX.train_distrib_"><code class="docutils literal notranslate"><span class="pre">train_distrib_</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.EDX.K_"><code class="docutils literal notranslate"><span class="pre">K_</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.EDX.verbose"><code class="docutils literal notranslate"><span class="pre">verbose</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.EDX.fit"><code class="docutils literal notranslate"><span class="pre">fit()</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.EDX.predict"><code class="docutils literal notranslate"><span class="pre">predict()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.EDy"><code class="docutils literal notranslate"><span class="pre">EDy</span></code></a><ul>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.EDy.estimator_train"><code class="docutils literal notranslate"><span class="pre">estimator_train</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.EDy.estimator_test"><code class="docutils literal notranslate"><span class="pre">estimator_test</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.EDy.predictions_train_"><code class="docutils literal notranslate"><span class="pre">predictions_train_</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.EDy.predictions_test_"><code class="docutils literal notranslate"><span class="pre">predictions_test_</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.EDy.needs_predictions_train"><code class="docutils literal notranslate"><span class="pre">needs_predictions_train</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.EDy.probabilistic_predictions"><code class="docutils literal notranslate"><span class="pre">probabilistic_predictions</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.EDy.classes_"><code class="docutils literal notranslate"><span class="pre">classes_</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.EDy.y_ext_"><code class="docutils literal notranslate"><span class="pre">y_ext_</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.EDy.train_n_cls_i_"><code class="docutils literal notranslate"><span class="pre">train_n_cls_i_</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.EDy.train_distrib_"><code class="docutils literal notranslate"><span class="pre">train_distrib_</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.EDy.K_"><code class="docutils literal notranslate"><span class="pre">K_</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.EDy.a_"><code class="docutils literal notranslate"><span class="pre">a_</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.EDy.verbose"><code class="docutils literal notranslate"><span class="pre">verbose</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.EDy.fit"><code class="docutils literal notranslate"><span class="pre">fit()</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.energy.EDy.predict"><code class="docutils literal notranslate"><span class="pre">predict()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-quantificationlib.multiclass.friedman">quantificationlib.multiclass.friedman module</a><ul>
<li><a class="reference internal" href="#quantificationlib.multiclass.friedman.FriedmanME"><code class="docutils literal notranslate"><span class="pre">FriedmanME</span></code></a><ul>
<li><a class="reference internal" href="#quantificationlib.multiclass.friedman.FriedmanME.estimator_train"><code class="docutils literal notranslate"><span class="pre">estimator_train</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.friedman.FriedmanME.estimator_test"><code class="docutils literal notranslate"><span class="pre">estimator_test</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.friedman.FriedmanME.predictions_train_"><code class="docutils literal notranslate"><span class="pre">predictions_train_</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.friedman.FriedmanME.predictions_test_"><code class="docutils literal notranslate"><span class="pre">predictions_test_</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.friedman.FriedmanME.needs_predictions_train"><code class="docutils literal notranslate"><span class="pre">needs_predictions_train</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.friedman.FriedmanME.probabilistic_predictions"><code class="docutils literal notranslate"><span class="pre">probabilistic_predictions</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.friedman.FriedmanME.distance"><code class="docutils literal notranslate"><span class="pre">distance</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.friedman.FriedmanME.tol"><code class="docutils literal notranslate"><span class="pre">tol</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.friedman.FriedmanME.classes_"><code class="docutils literal notranslate"><span class="pre">classes_</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.friedman.FriedmanME.y_ext_"><code class="docutils literal notranslate"><span class="pre">y_ext_</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.friedman.FriedmanME.train_prevs_"><code class="docutils literal notranslate"><span class="pre">train_prevs_</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.friedman.FriedmanME.train_distrib_"><code class="docutils literal notranslate"><span class="pre">train_distrib_</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.friedman.FriedmanME.test_distrib_"><code class="docutils literal notranslate"><span class="pre">test_distrib_</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.friedman.FriedmanME.problem_"><code class="docutils literal notranslate"><span class="pre">problem_</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.friedman.FriedmanME.mixtures_"><code class="docutils literal notranslate"><span class="pre">mixtures_</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.friedman.FriedmanME.verbose"><code class="docutils literal notranslate"><span class="pre">verbose</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.friedman.FriedmanME.fit"><code class="docutils literal notranslate"><span class="pre">fit()</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.friedman.FriedmanME.predict"><code class="docutils literal notranslate"><span class="pre">predict()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-quantificationlib.multiclass.knn">quantificationlib.multiclass.knn module</a><ul>
<li><a class="reference internal" href="#quantificationlib.multiclass.knn.PWKQuantifier"><code class="docutils literal notranslate"><span class="pre">PWKQuantifier</span></code></a><ul>
<li><a class="reference internal" href="#quantificationlib.multiclass.knn.PWKQuantifier.classes_"><code class="docutils literal notranslate"><span class="pre">classes_</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.knn.PWKQuantifier.cm_"><code class="docutils literal notranslate"><span class="pre">cm_</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.knn.PWKQuantifier.problem_"><code class="docutils literal notranslate"><span class="pre">problem_</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.knn.PWKQuantifier.verbose"><code class="docutils literal notranslate"><span class="pre">verbose</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.knn.PWKQuantifier.fit"><code class="docutils literal notranslate"><span class="pre">fit()</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.knn.PWKQuantifier.predict"><code class="docutils literal notranslate"><span class="pre">predict()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-quantificationlib.multiclass.regression">quantificationlib.multiclass.regression module</a><ul>
<li><a class="reference internal" href="#quantificationlib.multiclass.regression.REG"><code class="docutils literal notranslate"><span class="pre">REG</span></code></a><ul>
<li><a class="reference internal" href="#quantificationlib.multiclass.regression.REG.create_training_set_of_distributions"><code class="docutils literal notranslate"><span class="pre">create_training_set_of_distributions()</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.regression.REG.fit_regressor"><code class="docutils literal notranslate"><span class="pre">fit_regressor()</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.regression.REG.predict_bag"><code class="docutils literal notranslate"><span class="pre">predict_bag()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#quantificationlib.multiclass.regression.REGX"><code class="docutils literal notranslate"><span class="pre">REGX</span></code></a><ul>
<li><a class="reference internal" href="#quantificationlib.multiclass.regression.REGX.fit"><code class="docutils literal notranslate"><span class="pre">fit()</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.regression.REGX.predict"><code class="docutils literal notranslate"><span class="pre">predict()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#quantificationlib.multiclass.regression.REGy"><code class="docutils literal notranslate"><span class="pre">REGy</span></code></a><ul>
<li><a class="reference internal" href="#quantificationlib.multiclass.regression.REGy.fit"><code class="docutils literal notranslate"><span class="pre">fit()</span></code></a></li>
<li><a class="reference internal" href="#quantificationlib.multiclass.regression.REGy.predict"><code class="docutils literal notranslate"><span class="pre">predict()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-quantificationlib.multiclass">Module contents</a></li>
</ul>
</li>
</ul>




  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/quantificationlib.multiclass.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">Prueba QLIB Jaime 0.1 documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">quantificationlib.multiclass package</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2023, Jaime.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 6.1.3.
    </div>
  </body>
</html>
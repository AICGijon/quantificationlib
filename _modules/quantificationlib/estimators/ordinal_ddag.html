

<!doctype html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>quantificationlib.estimators.ordinal_ddag &#8212; quantificationlib 0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/bizstyle.css" />
    
    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/sphinx_highlight.js"></script>
    <script src="../../../_static/bizstyle.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0" />
    <!--[if lt IE 9]>
    <script src="_static/css3-mediaqueries.js"></script>
    <![endif]-->
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../../index.html">quantificationlib 0.1 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../../index.html" accesskey="U">Module code</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">quantificationlib.estimators.ordinal_ddag</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for quantificationlib.estimators.ordinal_ddag</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Estimator based on DDAGs (Decision Directed Acyclic Graphs)</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="c1"># Authors: Alberto Castaño &lt;bertocast@gmail.com&gt;</span>
<span class="c1">#          Pablo González &lt;gonzalezgpablo@uniovi.es&gt;</span>
<span class="c1">#          Jaime Alonso &lt;jalonso@uniovi.es&gt;</span>
<span class="c1">#          Juan José del Coz &lt;juanjo@uniovi.es&gt;</span>
<span class="c1"># License: BSD 3 clause, University of Oviedo</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">sklearn.exceptions</span> <span class="kn">import</span> <span class="n">NotFittedError</span>
<span class="kn">from</span> <span class="nn">sklearn.multiclass</span> <span class="kn">import</span> <span class="n">OneVsOneClassifier</span>


<div class="viewcode-block" id="DDAGClassifier"><a class="viewcode-back" href="../../../rsts_t4/quantificationlib.estimators.ordinal_ddag.html#quantificationlib.estimators.ordinal_ddag.DDAGClassifier">[docs]</a><span class="k">class</span> <span class="nc">DDAGClassifier</span><span class="p">(</span><span class="n">OneVsOneClassifier</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Decision Directed Acyclic Graph ordinal classifier</span>

<span class="sd">        This strategy consists on learning a classifier per each pair of classes, thus it requires to fit</span>
<span class="sd">        n_classes * (n_classes - 1) / 2 classifiers. For this reason, this class derives from OneVsOneClassifier and</span>
<span class="sd">        uses most of its functionalities, mainly to train the binary models.</span>

<span class="sd">        However, there are two main differences with respect to OneVsOneClassifier:</span>

<span class="sd">        1) This class is used for ordinal classification, it does not make sense to use it for multiclass classification</span>
<span class="sd">        2) The rule to make predictions is different. Here, the binary classifiers are arranged into a binary tree in</span>
<span class="sd">           which the classifier selected at each node is the one that deals with the two more distant remaining</span>
<span class="sd">           classes. Thus, the root contains the classifier that decides between the first class of the order and last</span>
<span class="sd">           class, and this idea is recursively applied.</span>

<span class="sd">            Example: in a ordinal classification problem with classes ranging from 1-star to 5-star,</span>
<span class="sd">            the corresponding DDAG will be</span>

<span class="sd">                                                      1|5</span>
<span class="sd">                                 1|4                                       2|5</span>
<span class="sd">                       1|3                 2|4                   2|4                  3|5</span>
<span class="sd">                  1|2       2|3       2|3       3|4         2|3       3|4        3|4       4|5</span>
<span class="sd">                1     2   2     3   2     3   3     4     2     3   3     4    3     4   4     5</span>

<span class="sd">            Since some subtrees are shared by different branches, for instance, the subtree labeled as node</span>
<span class="sd">            2|4 is shared by the right subtree of 1|4 and the left subtree of 2|5, the tree can be depicted in a more</span>
<span class="sd">            compact way:</span>
<span class="sd">                                                      1|5</span>
<span class="sd">                                              1|4             2|5</span>
<span class="sd">                                       1|3            2|4            3|5</span>
<span class="sd">                                 1|2          2|3             3|4           4|5</span>
<span class="sd">                             1          2              3              4             5</span>

<span class="sd">            in which all internal nodes (2|4, 2|3 and 3|4) and all leaves, except the first one, and the last one</span>
<span class="sd">            (2, 3 and 4) are reached from different paths.</span>


<span class="sd">        The class implements two different strategies to compute the probabilities for a given example:</span>

<span class="sd">        &#39;full_probabilistic&#39;</span>
<span class="sd">            The probabilities computed by each node are propagated through the tree. For those leaves that</span>
<span class="sd">            can be reached following different paths (all except the leaves for the first and the last class), the</span>
<span class="sd">            probabilities are summed. With this method, all the classes may have a probability greater that 0.</span>

<span class="sd">            Example: For a given example, the probability of the left class returned by each model is the following:</span>

<span class="sd">            P(1|5) = 0.2</span>
<span class="sd">            P(1|4) = 0.1</span>
<span class="sd">            P(2|5) = 0.1</span>
<span class="sd">            P(1|3) = 0.2</span>
<span class="sd">            P(2|4) = 0.3</span>
<span class="sd">            P(3|5) = 0.3</span>
<span class="sd">            P(1|2) = 0.3</span>
<span class="sd">            P(2|3) = 0.4</span>
<span class="sd">            P(3|4) = 0.4</span>
<span class="sd">            P(4|5) = 0.3</span>

<span class="sd">            P(y=1) = P(1|5) * P(1|4) * P(1|3) * P(1|2) =</span>
<span class="sd">                     0.2    * 0.1    * 0.2    * 0.3    = 0.0012</span>

<span class="sd">            P(y=2) = P(1|5)     * P(1|4)     * P(1|3)     * (1-P(1|2)) +</span>
<span class="sd">                     P(1|5)     * P(1|4)     * (1-P(1|3)) * P(2|3)     +</span>
<span class="sd">                     P(1|5)     * (1-P(1|4)) * P(2|4)     * P(2|3)     +</span>
<span class="sd">                     (1-P(1|5)) * P(2|5)     * P(2|4)     * P(2|3)     =</span>
<span class="sd">                     0.2        * 0.1        * 0.2        * 0.7        +</span>
<span class="sd">                     0.2        * 0.1        * 0.8        * 0.4        +</span>
<span class="sd">                     0.2        * 0.9        * 0.3        * 0.4        +</span>
<span class="sd">                     0.8        * 0.1        * 0.3        * 0.4        = 0.0028 + 0.0064 + 0.0216 + 0.0096 = 0.0404</span>

<span class="sd">            P(y=3) = P(1|5)     * P(1|4)     * (1-P(1|3)) * (1-P(2|3)) +</span>
<span class="sd">                     P(1|5)     * (1-P(1|4)) * P(2|4))    * (1-P(2|3)) +</span>
<span class="sd">                     P(1|5)     * (1-P(1|4)) * (1-P(2|4)) * P(3|4)     +</span>
<span class="sd">                     (1-P(1|5)) * P(2|5)     * P(2|4))    * (1-P(2|3)) +</span>
<span class="sd">                     (1-P(1|5)) * P(2|5)     * (1-P(2|4)) * P(3|4)     +</span>
<span class="sd">                     (1-P(1|5)) * (1-P(2|5)) * P(3|5)     * P(3|4)     =</span>
<span class="sd">                     0.2        * 0.1        * 0.8        * 0.6        +</span>
<span class="sd">                     0.2        * 0.9        * 0.3        * 0.6        +</span>
<span class="sd">                     0.2        * 0.9        * 0.7        * 0.4        +</span>
<span class="sd">                     0.8        * 0.1        * 0.3        * 0.6        +</span>
<span class="sd">                     0.8        * 0.1        * 0.7        * 0.4        +</span>
<span class="sd">                     0.8        * 0.9        * 0.3        * 0.4        = 0.0096 + 0.0324 + 0.0504 +</span>
<span class="sd">                                                                         0.0144 + 0.0224 + 0.0864 = 0.2156</span>
<span class="sd">            P(y=4) = P(1|5)     * (1-P(1|4)) * (1-P(2|4)) * (1-P(3|4)) +</span>
<span class="sd">                     (1-P(1|5)) * P(2|5)     * (1-P(2|4)) * (1-P(3|4)) +</span>
<span class="sd">                     (1-P(1|5)) * (1-P(2|5)) * P(3|5)     * (1-P(3|4)) +</span>
<span class="sd">                     (1-P(1|5)) * (1-P(2|5)) * (1-P(3|5)) * P(4|5)     =</span>
<span class="sd">                     0.2        * 0.9        * 0.7        * 0.6        + 0.0756</span>
<span class="sd">                     0.8        * 0.1        * 0.7        * 0.6        + 0.0336</span>
<span class="sd">                     0.8        * 0.9        * 0.3        * 0.6        + 0.1296</span>
<span class="sd">                     0.8        * 0.9        * 0.7        * 0.3        = 0.0756 + 0.0336 + 0.1296 + 0.1512 = 0.3900</span>

<span class="sd">            P(y=5) = (1-P(1|5)) * (1-P(2|5)) * (1-P(3|5)) * (1-P(4|5)) =</span>
<span class="sd">                     0.8        * 0.9        * 0.7        * 0.7        = 0.3528</span>

<span class="sd">            Thus, the probabilities returned by `predict_proba` method would be (0.0012, 0.0404, 02156, 0.3900, 0.3528)</span>

<span class="sd">        &#39;winner_node&#39;</span>
<span class="sd">            Uses the probabilities of binary estimators to descent until the level previous to the leaves (it is like</span>
<span class="sd">            binarizing such probabilities to 0,1). Then, the method returns the probalities of such binary estimator</span>
<span class="sd">            for the two consecutive classes involved, and zero for the rest of classes.</span>

<span class="sd">            Example: For a given example, the probability of the left class returned by each model is the following:</span>

<span class="sd">            P(1|5) = 0.2</span>
<span class="sd">            P(1|4) = 0.1</span>
<span class="sd">            P(2|5) = 0.1</span>
<span class="sd">            P(1|3) = 0.2</span>
<span class="sd">            P(2|4) = 0.3</span>
<span class="sd">            P(3|5) = 0.3</span>
<span class="sd">            P(1|2) = 0.3</span>
<span class="sd">            P(2|3) = 0.4</span>
<span class="sd">            P(3|4) = 0.4</span>
<span class="sd">            P(4|5) = 0.3</span>

<span class="sd">            Taking binary decisions from the root, we reach the binary classifier 4|5, thus the returned probabilities</span>
<span class="sd">            are (0, 0, 0, 0.3, 0.7)</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        estimator : estimator object (default=None)</span>
<span class="sd">            An estimator object implementing `fit` and one of `predict` or `predict_proba`. It is the base estimator</span>
<span class="sd">            used to learn the set of binary classifiers</span>

<span class="sd">        n_jobs : int or None, optional (default=None)</span>
<span class="sd">            The number of jobs to use for the computation.</span>
<span class="sd">            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.</span>
<span class="sd">            ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;`</span>
<span class="sd">            for more details.</span>

<span class="sd">        predict_method: str, optional (default &#39;full_probabilistic&#39;)</span>
<span class="sd">            &#39;full_probabilistic&#39;</span>
<span class="sd">            The probabilities computed by each node are propagated through the tree. For those leaves that</span>
<span class="sd">            can be reached following different paths (all except the leaves for the first and the last class), the</span>
<span class="sd">            probabilities are summed. With this method, all the classes may have a probability greater that 0.</span>

<span class="sd">            &#39;winner_node&#39;</span>
<span class="sd">            Uses the probabilities of binary estimators to descent until the level previous to the leaves (it is like</span>
<span class="sd">            binarizing such probabilities to 0,1). Then, the method returns the probalities of such binary estimator</span>
<span class="sd">            for the two consecutive classes involved, and zero for the rest of classes.</span>

<span class="sd">        verbose : int, optional, (default=0)</span>
<span class="sd">            The verbosity level. The default value, zero, means silent mode</span>

<span class="sd">        Attributes</span>
<span class="sd">        ----------</span>
<span class="sd">        estimator : estimator object</span>
<span class="sd">            An estimator object implementing `fit` and `predict_proba`.</span>

<span class="sd">        n_jobs : int or None,</span>
<span class="sd">            The number of jobs to use for the computation.</span>

<span class="sd">        predict_method: str</span>
<span class="sd">            The method used by `predict_proba` to compute the class probabilities of a given example</span>

<span class="sd">        verbose: int,</span>
<span class="sd">            The verbosity level.</span>

<span class="sd">        estimators_ : list of ``n_classes * (n_classes - 1) / 2`` estimators</span>
<span class="sd">            Estimators used for predictions.</span>

<span class="sd">        classes_ : numpy array of shape [n_classes]</span>
<span class="sd">            Array containing labels.</span>

<span class="sd">        References</span>
<span class="sd">        ----------</span>
<span class="sd">        José Ramón Quevedo, Elena Montañés, Óscar Luaces, Juan José del Coz: Adapting decision DAGs for multipartite</span>
<span class="sd">        ranking. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases (pp. 115-130).</span>
<span class="sd">        Springer, Berlin, Heidelberg.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">estimator</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">predict_method</span><span class="o">=</span><span class="s1">&#39;full_probabilistic&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DDAGClassifier</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">estimator</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">estimator</span> <span class="o">=</span> <span class="n">estimator</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span> <span class="o">=</span> <span class="n">n_jobs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">predict_method</span> <span class="o">=</span> <span class="n">predict_method</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="kc">None</span>

<div class="viewcode-block" id="DDAGClassifier.predict"><a class="viewcode-back" href="../../../rsts_t4/quantificationlib.estimators.ordinal_ddag.html#quantificationlib.estimators.ordinal_ddag.DDAGClassifier.predict">[docs]</a>    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Predict the class for each testing example</span>

<span class="sd">            The method computes the probability of each class (using `predict_proba`) and returns the class with</span>
<span class="sd">            highest probability</span>

<span class="sd">            Parameters</span>
<span class="sd">            ----------</span>
<span class="sd">            X : (sparse) array-like, shape (n_examples, n_features)</span>
<span class="sd">                Data</span>

<span class="sd">            Returns</span>
<span class="sd">            -------</span>
<span class="sd">            An array, shape(n_examples, ) with the predicted class for each example</span>

<span class="sd">            Raises</span>
<span class="sd">            ------</span>
<span class="sd">            NotFittedError</span>
<span class="sd">                When the estimators are not fitted yet</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">probs_samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">best_classes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">probs_samples</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">[</span><span class="n">best_classes</span><span class="p">]</span>  <span class="c1"># return best_classes is not correct</span></div>

<div class="viewcode-block" id="DDAGClassifier.predict_proba"><a class="viewcode-back" href="../../../rsts_t4/quantificationlib.estimators.ordinal_ddag.html#quantificationlib.estimators.ordinal_ddag.DDAGClassifier.predict_proba">[docs]</a>    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Predict the class probabilities for each example</span>

<span class="sd">            Two different methods are implemented depending on the value of `predict_method` attribute</span>

<span class="sd">            &#39;full_probabilistic&#39;</span>
<span class="sd">            The probabilities computed by each node are propagated through the tree. For those leaves that</span>
<span class="sd">            can be reached following different paths (all except the leaves for the first and the last class), the</span>
<span class="sd">            probabilities are summed. With this method, all the classes may have a probability greater that 0.</span>

<span class="sd">            &#39;winner_node&#39;</span>
<span class="sd">            Uses the probabilities of binary estimators to descent until the level previous to the leaves (it is like</span>
<span class="sd">            binarizing such probabilities to 0,1). Then, the method returns the probalities of such binary estimator</span>
<span class="sd">            for the two consecutive classes involved, and zero for the rest of classes.</span>

<span class="sd">            The method uses a recursive auxiliar method to compute the class probabilities</span>

<span class="sd">            Parameters</span>
<span class="sd">            ----------</span>
<span class="sd">            X : (sparse) array-like, shape (n_examples, n_features)</span>
<span class="sd">                Data</span>

<span class="sd">            Returns</span>
<span class="sd">            -------</span>
<span class="sd">            An array, shape(n_examples, n_classes) with the class probabilities for each example</span>

<span class="sd">            Raises</span>
<span class="sd">            ------</span>
<span class="sd">            NotFittedError</span>
<span class="sd">                When the estimators are not fitted yet</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">NotFittedError</span><span class="p">(</span><span class="s2">&quot;This instance of </span><span class="si">%s</span><span class="s2"> class is not fitted yet&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

        <span class="n">n_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>
        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">probs_ini</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">))</span>
        <span class="n">probs_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__compute_probabilities</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">probs_samples</span><span class="p">,</span> <span class="n">probs_accum</span><span class="o">=</span><span class="n">probs_ini</span><span class="p">,</span> <span class="n">left_class</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">right_class</span><span class="o">=</span><span class="n">n_classes</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">probs_samples</span></div>

    <span class="k">def</span> <span class="nf">__compute_probabilities</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">probs_samples</span><span class="p">,</span> <span class="n">probs_accum</span><span class="p">,</span> <span class="n">left_class</span><span class="p">,</span> <span class="n">right_class</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Compute the probability for each class applying the binary tree of models</span>

<span class="sd">            This method is recursive. It follows the binary tree of models, computing the probability of each class.</span>
<span class="sd">            The parameter probs_samples must be a matrix of zeros, shape(n_samples, n_classes). The method uses this</span>
<span class="sd">            matrix to compute the final class probabilities by adding the probabilities of the different paths for</span>
<span class="sd">            reaching each leaf. The class probabilities of each partial path are stored in probs_accum</span>

<span class="sd">            Parameters</span>
<span class="sd">            ----------</span>
<span class="sd">            X : (sparse) array-like, shape (n_examples, n_features)</span>
<span class="sd">                Data</span>

<span class="sd">            probs_samples : array, shape(n_examples, n_classes)</span>
<span class="sd">                The method computes the probabilities in this matrix. Initially must be set to ones. At the end of the</span>
<span class="sd">                recursion proccess this matrix contains the final probabilities</span>

<span class="sd">            probs_accum : array, shape(n_examples, n_classes)</span>
<span class="sd">                This matrix contains the accumulated probabilities from the root to the given node defined, by</span>
<span class="sd">                the parameters first and last</span>

<span class="sd">            left_class :  int, initially must be 0</span>
<span class="sd">                The index of the left class of the binary model at the current node</span>

<span class="sd">            right_class : int, initially must be equal to n_classes-1</span>
<span class="sd">                The index of the right class of the binary model at the current node</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">left_class</span> <span class="o">==</span> <span class="n">right_class</span><span class="p">:</span>
            <span class="c1"># it is a leaf, we have to add the probabilities of this branch (are stored in probs_accum</span>
            <span class="n">probs_samples</span> <span class="o">+=</span> <span class="n">probs_accum</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># internal node</span>
            <span class="n">n_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>

            <span class="n">pos_estim</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_classes</span> <span class="o">-</span> <span class="n">left_class</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">)))</span> <span class="o">+</span> <span class="p">(</span><span class="n">right_class</span> <span class="o">-</span> <span class="n">left_class</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[</span><span class="n">pos_estim</span><span class="p">]</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

            <span class="n">n_samples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>

            <span class="n">predictions_left</span> <span class="o">=</span> <span class="n">predictions</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
            <span class="n">predictions_right</span> <span class="o">=</span> <span class="n">predictions</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict_method</span> <span class="o">==</span> <span class="s1">&#39;winner_node&#39;</span> <span class="ow">and</span> <span class="n">right_class</span> <span class="o">-</span> <span class="n">left_class</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="c1"># binarizing the probabilities for the prediction method &#39;winner_node&#39;. This is done in all cases</span>
                <span class="c1"># except for the model previous to the leaves (when right_class-left_class == 1)</span>
                <span class="n">predictions_left</span><span class="p">[</span><span class="n">predictions_left</span> <span class="o">&lt;</span> <span class="mf">0.5</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">predictions_left</span><span class="p">[</span><span class="n">predictions_left</span> <span class="o">&gt;=</span> <span class="mf">0.5</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
                <span class="n">predictions_right</span><span class="p">[</span><span class="n">predictions_right</span> <span class="o">&lt;=</span> <span class="mf">0.5</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">predictions_right</span><span class="p">[</span><span class="n">predictions_right</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

            <span class="n">probs_left_accum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">))</span>
            <span class="n">probs_right_accum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">))</span>

            <span class="c1"># left side classes</span>
            <span class="k">for</span> <span class="n">cl</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">left_class</span><span class="p">,</span> <span class="n">right_class</span><span class="p">):</span>
                <span class="n">probs_left_accum</span><span class="p">[:,</span> <span class="n">cl</span><span class="p">]</span> <span class="o">=</span> <span class="n">probs_accum</span><span class="p">[:,</span> <span class="n">cl</span><span class="p">]</span> <span class="o">*</span> <span class="n">predictions_left</span>

            <span class="c1"># right side classes</span>
            <span class="k">for</span> <span class="n">cl</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">left_class</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">right_class</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
                <span class="n">probs_right_accum</span><span class="p">[:,</span> <span class="n">cl</span><span class="p">]</span> <span class="o">=</span> <span class="n">probs_accum</span><span class="p">[:,</span> <span class="n">cl</span><span class="p">]</span> <span class="o">*</span> <span class="n">predictions_right</span>

            <span class="c1"># process the probabilities of each subtree</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">__compute_probabilities</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">probs_samples</span><span class="p">,</span> <span class="n">probs_left_accum</span><span class="p">,</span> <span class="n">left_class</span><span class="p">,</span> <span class="n">right_class</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">__compute_probabilities</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">probs_samples</span><span class="p">,</span> <span class="n">probs_right_accum</span><span class="p">,</span> <span class="n">left_class</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">right_class</span><span class="p">)</span></div>

</pre></div>

            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">





<h3><a href="../../../index.html">Table of Contents</a></h3>




<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../../index.html">quantificationlib 0.1 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../../index.html" >Module code</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">quantificationlib.estimators.ordinal_ddag</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2023, AIC Gijón.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 6.1.3.
    </div>
  </body>
</html>